{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Natural Language Processing - Project part 1"],"metadata":{"id":"CBQI8RoG3jgX"}},{"cell_type":"markdown","source":["### Setup and Input"],"metadata":{"id":"88OAIqkI3tyb"}},{"cell_type":"code","source":["!pip install nltk\n","!pip install pandas\n","!pip install gensim\n","!pip install re\n","!pip install numpy\n","!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Xl3s_70k_-eO","executionInfo":{"status":"ok","timestamp":1760466253640,"user_tz":300,"elapsed":62276,"user":{"displayName":"Francesco d'angolo","userId":"01446161382200832800"}},"outputId":"09b6ed86-f863-4362-b30b-ebda94f8c67e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"code","source":["\n","import gensim\n","import numpy as np\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from gensim.downloader import load\n","import torch\n","import pandas as pd\n","from torch.utils.data import Dataset\n","import math"],"metadata":{"id":"x884ov5oEdm2","executionInfo":{"status":"ok","timestamp":1760466264461,"user_tz":300,"elapsed":10808,"user":{"displayName":"Francesco d'angolo","userId":"01446161382200832800"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import sys\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","DRIVE_NAME = \"NLPproject1\"\n","ROOT_PATH = os.path.join('/content/drive/MyDrive/', DRIVE_NAME)\n","SCRIPTS_PATH = f'{ROOT_PATH}/Scripts'\n","DATA_PATH = f'{ROOT_PATH}/Dataset'\n","\n","if SCRIPTS_PATH not in sys.path:\n","    sys.path.append(SCRIPTS_PATH)\n","\n","# --- Importa le tue classi personalizzate ---\n","from preprocessing import GloveEmbedder\n","from dataset import EmpathyDataset\n","from ann_model import DeepEmpathyNet\n","\n","print(\"âœ… Setup completato. Tutte le classi sono state importate.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17pYMxDr37oS","executionInfo":{"status":"ok","timestamp":1760466266779,"user_tz":300,"elapsed":2316,"user":{"displayName":"Francesco d'angolo","userId":"01446161382200832800"}},"outputId":"83fc6260-35a9-47ee-ca2b-ce21ab7af473"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","âœ… Setup completato. Tutte le classi sono state importate.\n"]}]},{"cell_type":"markdown","source":["### Data Preparation"],"metadata":{"id":"NKo5OxLjA14i"}},{"cell_type":"code","source":["# Embedder Initialization (GloVe model)\n","glove_embedder = GloveEmbedder(model_name=\"glove-wiki-gigaword-100\")\n","\n","# Data paths\n","train_csv_path = f\"{DATA_PATH}/trac2_CONVT_train.csv\"\n","eval_csv_path = f\"{DATA_PATH}/trac2_CONVT_dev.csv\"\n","\n","# Crea le istanze del Dataset\n","# La classe ora gestisce tutto internamente: caricamento, pulizia, embedding!\n","print(\"\\n--- Training Dataset Creation ---\")\n","train_dataset = EmpathyDataset(csv_path=train_csv_path, embedder=glove_embedder)\n","\n","print(\"\\n--- Evaluation Dataset Creation ---\")\n","eval_dataset = EmpathyDataset(csv_path=eval_csv_path, embedder=glove_embedder)\n","\n","# DataLoaders\n","BATCH_SIZE = 32\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","print(f\"\\nâœ… DataLoaders successfully created. Batch size: {BATCH_SIZE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kMJzWeGYAcMP","executionInfo":{"status":"ok","timestamp":1760466324041,"user_tz":300,"elapsed":57260,"user":{"displayName":"Francesco d'angolo","userId":"01446161382200832800"}},"outputId":"98495d9e-d772-4459-b24b-df24e92d1d2c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Caricamento del modello GloVe 'glove-wiki-gigaword-100'...\n","Modello GloVe caricato con successo.\n","\n","--- Training Dataset Creation ---\n","Loaded and cleaned data from /content/drive/MyDrive/NLPproject1/Dataset/trac2_CONVT_train.csv. Number of samples:11090\n","Generating embeddings for 11090 samples...\n","\n","--- Evaluation Dataset Creation ---\n","Warning: The '3' polarity class is not present in this dataset.\n","Loaded and cleaned data from /content/drive/MyDrive/NLPproject1/Dataset/trac2_CONVT_dev.csv. Number of samples:987\n","Generating embeddings for 987 samples...\n","\n","âœ… DataLoaders successfully created. Batch size: 32\n"]}]},{"cell_type":"markdown","source":["### Model Initialization\n"],"metadata":{"id":"n0LIigKcHGSM"}},{"cell_type":"code","source":["experiment_configs = [\n","    {\n","        \"experiment_name\": \"baseline_relu_dropout_0.3\",\n","        \"input_dim\": glove_embedder.vector_size, # Should be 100\n","        \"hidden_dims\": [256, 128],\n","        \"dropout\": 0.3,\n","        \"activation\": \"relu\",\n","        \"norm_type\": \"layernorm\",\n","        \"num_classes\": 4,\n","        \"learning_rate\": 1e-3,\n","    },\n","    {\n","        \"experiment_name\": \"deep_gelu_dropout_0.5\",\n","        \"input_dim\": glove_embedder.vector_size,\n","        \"hidden_dims\": [512, 256, 128], # Rete piÃ¹ profonda\n","        \"dropout\": 0.5,                  # Dropout piÃ¹ aggressivo\n","        \"activation\": \"gelu\",            # Attivazione diversa\n","        \"norm_type\": \"batchnorm\",\n","        \"num_classes\": 4,\n","        \"learning_rate\": 1e-4,           # Learning rate piÃ¹ basso\n","    },\n","    {\n","        \"experiment_name\": \"shallow_leakyrelu_low_dropout\",\n","        \"input_dim\": glove_embedder.vector_size,\n","        \"hidden_dims\": [512],           # Rete piÃ¹ superficiale\n","        \"dropout\": 0.2,\n","        \"activation\": \"leakyrelu\",\n","        \"norm_type\": \"layernorm\",\n","        \"num_classes\": 4,\n","        \"learning_rate\": 1e-3,\n","    }\n","    # Add other configs\n","]\n","\n","NUM_EPOCHS = 10\n","MODELS_SAVE_PATH = f\"{ROOT_PATH}/Saved Models\"\n","os.makedirs(MODELS_SAVE_PATH, exist_ok=True)\n","\n","print(f\"Read {len(experiment_configs)} esperimenti.\")\n","print(f\"I modelli migliori verranno salvati in: {MODELS_SAVE_PATH}\")"],"metadata":{"id":"FYdzl53YHPUy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760466324056,"user_tz":300,"elapsed":6,"user":{"displayName":"Francesco d'angolo","userId":"01446161382200832800"}},"outputId":"92ae8014-bafd-4780-d5f2-c456975d55b5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Read 3 esperimenti.\n","I modelli migliori verranno salvati in: /content/drive/MyDrive/NLPproject1/Saved Models\n"]}]},{"cell_type":"markdown","source":["### Training and Evaluation"],"metadata":{"id":"TDk09uLAji7R"}},{"cell_type":"code","source":["import torch.optim as optim\n","import torch.nn as nn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","def train_one_epoch(model, dataloader, optimizer, loss_fns, device):\n","    model.train()\n","    total_loss = 0.0\n","\n","    # Definisci i pesi per ogni loss (puoi regolarli)\n","    loss_weights = {'intensity': 0.2, 'empathy': 0.2, 'polarity': 0.6}\n","\n","    for batch in dataloader:\n","        features = batch['features'].to(device)\n","        labels = {k: v.to(device) for k, v in batch['labels'].items()}\n","\n","        # Forward pass\n","        outputs = model(features)\n","\n","        # Calcolo delle loss\n","        loss_intensity = loss_fns['regression'](outputs['intensity'], labels['intensity'])\n","        loss_empathy = loss_fns['regression'](outputs['empathy'], labels['empathy'])\n","        loss_polarity = loss_fns['classification'](outputs['polarity'], labels['polarity'])\n","\n","        # Loss combinata e pesata\n","        combined_loss = (loss_weights['intensity'] * loss_intensity +\n","                         loss_weights['empathy'] * loss_empathy +\n","                         loss_weights['polarity'] * loss_polarity)\n","\n","        # Backward pass e ottimizzazione\n","        optimizer.zero_grad()\n","        combined_loss.backward()\n","        optimizer.step()\n","\n","        total_loss += combined_loss.item()\n","\n","    return total_loss / len(dataloader)\n","\n","def evaluate_performance(model, dataloader, loss_fns, device):\n","    model.eval()\n","    total_loss = 0.0\n","    loss_weights = {'intensity': 0.2, 'empathy': 0.2, 'polarity': 0.6}\n","\n","    all_intensity_preds, all_intensity_labels = [], []\n","    all_empathy_preds, all_empathy_labels = [], []\n","    all_polarity_preds, all_polarity_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            features = batch['features'].to(device)\n","            labels = {k: v.to(device) for k, v in batch['labels'].items()}\n","\n","            outputs = model(features)\n","\n","            loss_intensity = loss_fns['regression'](outputs['intensity'], labels['intensity'])\n","            loss_empathy = loss_fns['regression'](outputs['empathy'], labels['empathy'])\n","            loss_polarity = loss_fns['classification'](outputs['polarity'], labels['polarity'])\n","            combined_loss = (loss_weights['intensity'] * loss_intensity +\n","                             loss_weights['empathy'] * loss_empathy +\n","                             loss_weights['polarity'] * loss_polarity)\n","            total_loss += combined_loss.item()\n","\n","            # --- Salvataggio delle predizioni per il calcolo delle metriche ---\n","            # Regressione\n","            all_intensity_preds.append(outputs['intensity'].cpu())\n","            all_intensity_labels.append(labels['intensity'].cpu())\n","            all_empathy_preds.append(outputs['empathy'].cpu())\n","            all_empathy_labels.append(labels['empathy'].cpu())\n","\n","            # Classificazione\n","            polarity_preds = torch.argmax(outputs['polarity'], dim=1)\n","            all_polarity_preds.append(polarity_preds.cpu())\n","            all_polarity_labels.append(labels['polarity'].cpu())\n","\n","    # Concatena i risultati di tutti i batch\n","    all_intensity_preds = torch.cat(all_intensity_preds)\n","    all_intensity_labels = torch.cat(all_intensity_labels)\n","    all_empathy_preds = torch.cat(all_empathy_preds)\n","    all_empathy_labels = torch.cat(all_empathy_labels)\n","    all_polarity_preds = torch.cat(all_polarity_preds)\n","    all_polarity_labels = torch.cat(all_polarity_labels)\n","\n","    # --- Calcolo delle Metriche ---\n","    # MAE per regressione (usando la funzione L1 Loss di PyTorch)\n","    mae_intensity = nn.functional.l1_loss(all_intensity_preds, all_intensity_labels).item()\n","    mae_empathy = nn.functional.l1_loss(all_empathy_preds, all_empathy_labels).item()\n","\n","    # Metriche di classificazione (usando scikit-learn)\n","    accuracy_polarity = accuracy_score(all_polarity_labels, all_polarity_preds)\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        all_polarity_labels, all_polarity_preds, average='weighted', zero_division=0\n","    )\n","\n","    # Ritorna un dizionario con tutte le metriche\n","    metrics = {\n","        \"val_loss\": total_loss / len(dataloader),\n","        \"intensity_mae\": mae_intensity,\n","        \"empathy_mae\": mae_empathy,\n","        \"polarity_accuracy\": accuracy_polarity,\n","        \"polarity_precision\": precision,\n","        \"polarity_recall\": recall,\n","        \"polarity_f1\": f1\n","    }\n","\n","    return metrics\n","\n","print(\"âœ… Training and evaluation function defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXi5wpzOjk0x","executionInfo":{"status":"ok","timestamp":1760466324083,"user_tz":300,"elapsed":25,"user":{"displayName":"Francesco d'angolo","userId":"01446161382200832800"}},"outputId":"5fd24747-b846-43e6-f92d-661415d9e3f9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Training and evaluation function defined.\n"]}]},{"cell_type":"code","source":["# ============= #\n","# EXPERIMENT 1  #\n","# ============= #\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","results = []\n","best_overall_val_loss = float('inf')\n","\n","for config in experiment_configs:\n","    print(f\"\\n{'='*20} START EXPERIMENT: {config['experiment_name']} {'='*20}\")\n","\n","    model = DeepEmpathyNet(config).to(device)\n","\n","    loss_functions = {\n","        'regression': nn.MSELoss(),\n","        'classification': nn.CrossEntropyLoss()\n","    }\n","    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n","\n","    best_epoch_metrics = {\"val_loss\": float('inf')}\n","\n","    for epoch in range(NUM_EPOCHS):\n","        train_loss = train_one_epoch(model, train_loader, optimizer, loss_functions, device)\n","        val_metrics = evaluate_performance(model, eval_loader, loss_functions, device)\n","\n","        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} -> Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['val_loss']:.4f} | \"\n","              f\"Polarity F1: {val_metrics['polarity_f1']:.4f} | Intensity MAE: {val_metrics['intensity_mae']:.4f}\")\n","\n","        # Salva il modello basandoti sulla validation loss\n","        if val_metrics['val_loss'] < best_epoch_metrics['val_loss']:\n","            best_epoch_metrics = val_metrics # Salva tutte le metriche della migliore epoca\n","            model_save_path = os.path.join(MODELS_SAVE_PATH, f\"{config['experiment_name']}_best.pth\")\n","            torch.save(model.state_dict(), model_save_path)\n","            print(f\"  -> New best model for this experiment! Saved in {model_save_path}\")\n","\n","    # Aggiungi tutte le metriche della migliore epoca ai risultati finali\n","    final_result = {\"experiment_name\": config['experiment_name'], \"model_path\": model_save_path}\n","    final_result.update(best_epoch_metrics)\n","    results.append(final_result)\n","\n","    if best_epoch_metrics['val_loss'] < best_overall_val_loss:\n","        best_overall_val_loss = best_epoch_metrics['val_loss']\n","        print(f\"\\nNEW BEST MODEL OVERALL: {config['experiment_name']} ðŸ†\")\n","\n","print(f\"\\n{'='*20} END OF ALL EXPERIMENTS {'='*20}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"lmxA9sX93Uni","executionInfo":{"status":"ok","timestamp":1760466391118,"user_tz":300,"elapsed":67032,"user":{"displayName":"Francesco d'angolo","userId":"01446161382200832800"}},"outputId":"edfc262f-763b-4c3c-ea7e-c261b12f246a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================== START EXPERIMENT: baseline_relu_dropout_0.3 ====================\n","Epoch 1/10 -> Train Loss: 0.8119 | Val Loss: 0.7568 | Polarity F1: 0.6156 | Intensity MAE: 0.5231\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n","Epoch 2/10 -> Train Loss: 0.7127 | Val Loss: 0.7426 | Polarity F1: 0.6191 | Intensity MAE: 0.5334\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n","Epoch 3/10 -> Train Loss: 0.6964 | Val Loss: 0.7388 | Polarity F1: 0.6395 | Intensity MAE: 0.5110\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n","Epoch 4/10 -> Train Loss: 0.6876 | Val Loss: 0.7307 | Polarity F1: 0.6388 | Intensity MAE: 0.5086\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n","Epoch 5/10 -> Train Loss: 0.6751 | Val Loss: 0.7355 | Polarity F1: 0.6562 | Intensity MAE: 0.5461\n","Epoch 6/10 -> Train Loss: 0.6700 | Val Loss: 0.7309 | Polarity F1: 0.6456 | Intensity MAE: 0.5534\n","Epoch 7/10 -> Train Loss: 0.6625 | Val Loss: 0.7252 | Polarity F1: 0.6369 | Intensity MAE: 0.5061\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n","Epoch 8/10 -> Train Loss: 0.6584 | Val Loss: 0.7211 | Polarity F1: 0.6477 | Intensity MAE: 0.5169\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n","Epoch 9/10 -> Train Loss: 0.6528 | Val Loss: 0.7275 | Polarity F1: 0.6262 | Intensity MAE: 0.5011\n","Epoch 10/10 -> Train Loss: 0.6499 | Val Loss: 0.7133 | Polarity F1: 0.6438 | Intensity MAE: 0.5059\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n","\n","NEW BEST MODEL OVERALL: baseline_relu_dropout_0.3 ðŸ†\n","\n","==================== START EXPERIMENT: deep_gelu_dropout_0.5 ====================\n","Epoch 1/10 -> Train Loss: 2.0414 | Val Loss: 1.5461 | Polarity F1: 0.5642 | Intensity MAE: 1.3405\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n","Epoch 2/10 -> Train Loss: 1.1363 | Val Loss: 1.0506 | Polarity F1: 0.5794 | Intensity MAE: 0.8387\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n","Epoch 3/10 -> Train Loss: 0.9277 | Val Loss: 0.8841 | Polarity F1: 0.5734 | Intensity MAE: 0.6466\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n","Epoch 4/10 -> Train Loss: 0.8878 | Val Loss: 0.8642 | Polarity F1: 0.6077 | Intensity MAE: 0.6646\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n","Epoch 5/10 -> Train Loss: 0.8643 | Val Loss: 0.8224 | Polarity F1: 0.6221 | Intensity MAE: 0.6291\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n","Epoch 6/10 -> Train Loss: 0.8367 | Val Loss: 0.8269 | Polarity F1: 0.6234 | Intensity MAE: 0.6210\n","Epoch 7/10 -> Train Loss: 0.8317 | Val Loss: 0.7960 | Polarity F1: 0.6243 | Intensity MAE: 0.6009\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n","Epoch 8/10 -> Train Loss: 0.8114 | Val Loss: 0.8161 | Polarity F1: 0.6251 | Intensity MAE: 0.6126\n","Epoch 9/10 -> Train Loss: 0.7970 | Val Loss: 0.7938 | Polarity F1: 0.6297 | Intensity MAE: 0.6117\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n","Epoch 10/10 -> Train Loss: 0.7919 | Val Loss: 0.7852 | Polarity F1: 0.6343 | Intensity MAE: 0.6123\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n","\n","==================== START EXPERIMENT: shallow_leakyrelu_low_dropout ====================\n","Epoch 1/10 -> Train Loss: 0.7618 | Val Loss: 0.7304 | Polarity F1: 0.6278 | Intensity MAE: 0.5588\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/shallow_leakyrelu_low_dropout_best.pth\n","Epoch 2/10 -> Train Loss: 0.6908 | Val Loss: 0.7337 | Polarity F1: 0.6411 | Intensity MAE: 0.5525\n","Epoch 3/10 -> Train Loss: 0.6709 | Val Loss: 0.7228 | Polarity F1: 0.6363 | Intensity MAE: 0.5504\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/shallow_leakyrelu_low_dropout_best.pth\n","Epoch 4/10 -> Train Loss: 0.6591 | Val Loss: 0.7250 | Polarity F1: 0.6237 | Intensity MAE: 0.5100\n","Epoch 5/10 -> Train Loss: 0.6495 | Val Loss: 0.8202 | Polarity F1: 0.5838 | Intensity MAE: 0.6253\n","Epoch 6/10 -> Train Loss: 0.6420 | Val Loss: 0.7608 | Polarity F1: 0.6266 | Intensity MAE: 0.6075\n","Epoch 7/10 -> Train Loss: 0.6313 | Val Loss: 0.7478 | Polarity F1: 0.6192 | Intensity MAE: 0.5338\n","Epoch 8/10 -> Train Loss: 0.6238 | Val Loss: 0.7202 | Polarity F1: 0.6428 | Intensity MAE: 0.5225\n","  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/shallow_leakyrelu_low_dropout_best.pth\n","Epoch 9/10 -> Train Loss: 0.6116 | Val Loss: 0.7333 | Polarity F1: 0.6345 | Intensity MAE: 0.5322\n","Epoch 10/10 -> Train Loss: 0.6020 | Val Loss: 0.7502 | Polarity F1: 0.6217 | Intensity MAE: 0.5369\n","\n","==================== END OF ALL EXPERIMENTS ====================\n"]}]},{"cell_type":"markdown","source":["### Analysis Results"],"metadata":{"id":"jGf1qwHhl3bt"}},{"cell_type":"code","source":["from datetime import datetime\n","import pandas as pd\n","\n","REPORT_PATH = os.path.join(ROOT_PATH, 'Report')\n","os.makedirs(REPORT_PATH, exist_ok=True)\n","\n","timestamp = datetime.now().strftime(\"%Y%m%d\")\n","report_filename = f\"experiment_results_{timestamp}.csv\"\n","report_filepath = os.path.join(REPORT_PATH, report_filename)\n","\n","results_df = pd.DataFrame(results)\n","\n","# Seleziona e Rinomina le colonne per una migliore leggibilitÃ \n","display_columns = {\n","    'experiment_name': 'Experiment',\n","    'val_loss': 'Validation Loss',\n","    'polarity_f1': 'Polarity F1',\n","    'polarity_accuracy': 'Polarity Acc.',\n","    'intensity_mae': 'Intensity MAE',\n","    'empathy_mae': 'Empathy MAE'\n","}\n","results_df_display = results_df[list(display_columns.keys())].rename(columns=display_columns)\n","results_df_sorted = results_df_display.sort_values(by=\"Validation Loss\", ascending=True).reset_index(drop=True)\n","\n","try:\n","    results_df_sorted.to_csv(report_filepath, index=False)\n","    print(f\"Report salvato con successo!\")\n","    print(f\"Percorso del file: {report_filepath}\")\n","except Exception as e:\n","    print(f\"Errore durante il salvataggio del report: {e}\")\n","\n","print(\"\\n--- Report preview ---\")\n","saved_report = pd.read_csv(report_filepath)\n","print(saved_report.head())\n","\n","best_experiment_name = results_df_sorted.iloc[0]['Experiment']\n","best_experiment_full_info = next(item for item in results if item[\"experiment_name\"] == best_experiment_name)\n","\n","print(f\"\\nThe best experiment is '{best_experiment_full_info['experiment_name']}'\")\n","print(f\"with a validation loss: {best_experiment_full_info['val_loss']:.4f}\")\n","print(f\"The corresponding model is saved in: {best_experiment_full_info['model_path']}\")\n","\n","\n","# Per caricare il modello migliore in futuro, puoi usare:\n","# model_config = ... (la configurazione del miglior esperimento)\n","# best_model = DeepEmpathyNet(model_config)\n","# best_model.load_state_dict(torch.load(best_experiment['model_path']))\n","# best_model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpxPreeZ5BdJ","executionInfo":{"status":"ok","timestamp":1760466391206,"user_tz":300,"elapsed":68,"user":{"displayName":"Francesco d'angolo","userId":"01446161382200832800"}},"outputId":"0c510b0b-6d53-4c6d-9758-23c40862bbfe"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Report salvato con successo!\n","Percorso del file: /content/drive/MyDrive/NLPproject1/Report/experiment_results_20251014.csv\n","\n","--- Report preview ---\n","                      Experiment  Validation Loss  Polarity F1  Polarity Acc.  \\\n","0      baseline_relu_dropout_0.3         0.713343     0.643797       0.654509   \n","1  shallow_leakyrelu_low_dropout         0.720213     0.642761       0.656535   \n","2          deep_gelu_dropout_0.5         0.785186     0.634318       0.651469   \n","\n","   Intensity MAE  Empathy MAE  \n","0       0.505874     0.745510  \n","1       0.522459     0.758093  \n","2       0.612291     0.772994  \n","\n","The best experiment is 'baseline_relu_dropout_0.3'\n","with a validation loss: 0.7133\n","The corresponding model is saved in: /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n"]}]},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"TSvn7YY_7yqz"}},{"cell_type":"code","source":["from dataset import InferenceDataset\n","\n","TEST_CSV_PATH = f\"{DATA_PATH}/trac2_CONVT_test.csv\"\n","SUBMISSION_PATH = os.path.join(REPORT_PATH, 'ann_report.csv')\n","ID_COLUMN_NAME = 'id'       # Verifica il nome della colonna ID nel file di test\n","TEXT_COLUMN_NAME = 'text'   # Verifica il nome della colonna di testo\n","\n","print(\"--- Start inference on test set ---\")\n","\n","# Recuperiamo la configurazione e il percorso del miglior esperimento\n","best_experiment_name = results_df_sorted.iloc[0]['Experiment']\n","best_experiment_info = next(item for item in experiment_configs if item[\"experiment_name\"] == best_experiment_name)\n","best_model_path = os.path.join(MODELS_SAVE_PATH, f\"{best_experiment_name}_best.pth\")\n","\n","print(f\"Best model: '{best_experiment_name}'\")\n","model = DeepEmpathyNet(best_experiment_info).to(device)\n","model.load_state_dict(torch.load(best_model_path))\n","model.eval()\n","\n","\n","test_dataset = InferenceDataset(\n","    csv_path=TEST_CSV_PATH,\n","    embedder=glove_embedder,\n","    id_column=ID_COLUMN_NAME,\n","    text_column=TEXT_COLUMN_NAME\n",")\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # shuffle=False Ã¨ importante!\n","\n","\n","all_ids = []\n","all_emotion_preds = []\n","all_empathy_preds = []\n","all_polarity_preds = []\n","\n","print(\"Predictions running...\")\n","with torch.no_grad(): # Disabilita il calcolo dei gradienti per velocizzare\n","    for batch in test_loader:\n","        ids = batch['id']\n","        features = batch['features'].to(device)\n","\n","        outputs = model(features)\n","\n","        emotion_preds = outputs['intensity'].squeeze().cpu().numpy()\n","        empathy_preds = outputs['empathy'].squeeze().cpu().numpy()\n","        polarity_preds = torch.argmax(outputs['polarity'], dim=1).cpu().numpy()\n","\n","        all_ids.extend(ids.numpy())\n","        all_emotion_preds.extend(emotion_preds)\n","        all_empathy_preds.extend(empathy_preds)\n","        all_polarity_preds.extend(polarity_preds)\n","\n","print(\"Predictions completed.\")\n","\n","submission_df = pd.DataFrame({\n","    'id': all_ids,\n","    'Emotion': all_emotion_preds,\n","    'EmotionalPolarity': all_polarity_preds,\n","    'Empathy': all_empathy_preds\n","})\n","\n","submission_df['EmotionalPolarity'] = submission_df['EmotionalPolarity'].astype(int)\n","\n","\n","submission_df.to_csv(SUBMISSION_PATH, index=False)\n","\n","print(f\"\\nâœ… File di submission creato con successo!\")\n","print(f\"Percorso del file: {SUBMISSION_PATH}\")\n","\n","print(\"\\n--- Anteprima del file di submission ---\")\n","print(submission_df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9eh5EuX70JZ","executionInfo":{"status":"ok","timestamp":1760466391729,"user_tz":300,"elapsed":529,"user":{"displayName":"Francesco d'angolo","userId":"01446161382200832800"}},"outputId":"5173dba9-def0-4ec8-9017-6ea4f53dde5e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Start inference on test set ---\n","Best model: 'baseline_relu_dropout_0.3'\n","Generazione degli embedding per 2311 campioni di test...\n","Predictions running...\n","Predictions completed.\n","\n","âœ… File di submission creato con successo!\n","Percorso del file: /content/drive/MyDrive/NLPproject1/Report/ann_report.csv\n","\n","--- Anteprima del file di submission ---\n","   id   Emotion  EmotionalPolarity   Empathy\n","0   1  2.195595                  2  2.051561\n","1   2  2.463433                  2  2.298273\n","2   3  2.412533                  2  2.224228\n","3   4  2.161638                  2  2.134687\n","4   5  2.682936                  2  2.594292\n"]}]}]}