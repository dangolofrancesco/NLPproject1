{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBQI8RoG3jgX"
   },
   "source": [
    "# Natural Language Processing - Project part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88OAIqkI3tyb"
   },
   "source": [
    "### Setup and Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 10808,
     "status": "ok",
     "timestamp": 1760466264461,
     "user": {
      "displayName": "Francesco d'angolo",
      "userId": "01446161382200832800"
     },
     "user_tz": 300
    },
    "id": "x884ov5oEdm2"
   },
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.downloader import load\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2316,
     "status": "ok",
     "timestamp": 1760466266779,
     "user": {
      "displayName": "Francesco d'angolo",
      "userId": "01446161382200832800"
     },
     "user_tz": 300
    },
    "id": "17pYMxDr37oS",
    "outputId": "83fc6260-35a9-47ee-ca2b-ce21ab7af473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "✅ Setup completato. Tutte le classi sono state importate.\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# This cell works only on colab  #\n",
    "##################################\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "DRIVE_NAME = \"NLPproject1\"\n",
    "ROOT_PATH = os.path.join('/content/drive/MyDrive/', DRIVE_NAME)\n",
    "SCRIPTS_PATH = f'{ROOT_PATH}/Scripts'\n",
    "DATA_PATH = f'{ROOT_PATH}/Dataset'\n",
    "\n",
    "if SCRIPTS_PATH not in sys.path:\n",
    "    sys.path.append(SCRIPTS_PATH)\n",
    "\n",
    "# --- Importa le tue classi personalizzate ---\n",
    "from preprocessing import GloveEmbedder\n",
    "from dataset import EmpathyDataset\n",
    "from ann_model import DeepEmpathyNet\n",
    "\n",
    "print(\"Setup completed. All classes imported successfully.\")\n",
    "print(f\"Root Path: {ROOT_PATH}\")\n",
    "print(f\"Scripts Path: {SCRIPTS_PATH}\")\n",
    "print(f\"Data Path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completed. All classes imported successfully.\n",
      "Root Path: /Users/francescodangolo/Downloads/NLPproject1\n",
      "Scripts Path: /Users/francescodangolo/Downloads/NLPproject1/Scripts\n",
      "Data Path: /Users/francescodangolo/Downloads/NLPproject1/Dataset\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# This cell works for VS Code    #\n",
    "##################################\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up paths for local development\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "SCRIPTS_PATH = os.path.join(ROOT_PATH, 'Scripts')\n",
    "DATA_PATH = os.path.join(ROOT_PATH, 'Dataset')\n",
    "\n",
    "if SCRIPTS_PATH not in sys.path:\n",
    "    sys.path.append(SCRIPTS_PATH)\n",
    "\n",
    "# Import custom classes\n",
    "from preprocessing import GloveEmbedder\n",
    "from dataset import EmpathyDataset\n",
    "from ann_model import DeepEmpathyNet\n",
    "\n",
    "print(\"Setup completed. All classes imported successfully.\")\n",
    "print(f\"Root Path: {ROOT_PATH}\")\n",
    "print(f\"Scripts Path: {SCRIPTS_PATH}\")\n",
    "print(f\"Data Path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKo5OxLjA14i"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57260,
     "status": "ok",
     "timestamp": 1760466324041,
     "user": {
      "displayName": "Francesco d'angolo",
      "userId": "01446161382200832800"
     },
     "user_tz": 300
    },
    "id": "kMJzWeGYAcMP",
    "outputId": "98495d9e-d772-4459-b24b-df24e92d1d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello GloVe 'glove-wiki-gigaword-100'...\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "\n",
      "Modello GloVe caricato con successo.\n",
      "\n",
      "--- Training Dataset Creation ---\n",
      "Loaded and cleaned data from /Users/francescodangolo/Downloads/NLPproject1/Dataset/trac2_CONVT_train.csv. Number of samples:11090\n",
      "Generating embeddings for 11090 samples...\n",
      "Modello GloVe caricato con successo.\n",
      "\n",
      "--- Training Dataset Creation ---\n",
      "Loaded and cleaned data from /Users/francescodangolo/Downloads/NLPproject1/Dataset/trac2_CONVT_train.csv. Number of samples:11090\n",
      "Generating embeddings for 11090 samples...\n",
      "\n",
      "--- Evaluation Dataset Creation ---\n",
      "Warning: The '3' polarity class is not present in this dataset.\n",
      "Loaded and cleaned data from /Users/francescodangolo/Downloads/NLPproject1/Dataset/trac2_CONVT_dev.csv. Number of samples:987\n",
      "Generating embeddings for 987 samples...\n",
      "\n",
      "✅ DataLoaders successfully created. Batch size: 32\n",
      "\n",
      "--- Evaluation Dataset Creation ---\n",
      "Warning: The '3' polarity class is not present in this dataset.\n",
      "Loaded and cleaned data from /Users/francescodangolo/Downloads/NLPproject1/Dataset/trac2_CONVT_dev.csv. Number of samples:987\n",
      "Generating embeddings for 987 samples...\n",
      "\n",
      "✅ DataLoaders successfully created. Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Embedder Initialization (GloVe model)\n",
    "glove_embedder = GloveEmbedder(model_name=\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Data paths\n",
    "train_csv_path = f\"{DATA_PATH}/trac2_CONVT_train.csv\"\n",
    "eval_csv_path = f\"{DATA_PATH}/trac2_CONVT_dev.csv\"\n",
    "\n",
    "# Crea le istanze del Dataset\n",
    "# La classe ora gestisce tutto internamente: caricamento, pulizia, embedding!\n",
    "print(\"\\n--- Training Dataset Creation ---\")\n",
    "train_dataset = EmpathyDataset(csv_path=train_csv_path, embedder=glove_embedder)\n",
    "\n",
    "print(\"\\n--- Evaluation Dataset Creation ---\")\n",
    "eval_dataset = EmpathyDataset(csv_path=eval_csv_path, embedder=glove_embedder)\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\n✅ DataLoaders successfully created. Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0LIigKcHGSM"
   },
   "source": [
    "### Model Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1760466324056,
     "user": {
      "displayName": "Francesco d'angolo",
      "userId": "01446161382200832800"
     },
     "user_tz": 300
    },
    "id": "FYdzl53YHPUy",
    "outputId": "92ae8014-bafd-4780-d5f2-c456975d55b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 3 esperimenti.\n",
      "I modelli migliori verranno salvati in: /content/drive/MyDrive/NLPproject1/Saved Models\n"
     ]
    }
   ],
   "source": [
    "experiment_configs = [\n",
    "    {\n",
    "        \"experiment_name\": \"baseline_relu_dropout_0.3\",\n",
    "        \"input_dim\": glove_embedder.vector_size, # Should be 100\n",
    "        \"hidden_dims\": [256, 128],\n",
    "        \"dropout\": 0.3,\n",
    "        \"activation\": \"relu\",\n",
    "        \"norm_type\": \"layernorm\",\n",
    "        \"num_classes\": 4,\n",
    "        \"learning_rate\": 1e-3,\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"deep_gelu_dropout_0.5\",\n",
    "        \"input_dim\": glove_embedder.vector_size,\n",
    "        \"hidden_dims\": [512, 256, 128], # Rete più profonda\n",
    "        \"dropout\": 0.5,                  # Dropout più aggressivo\n",
    "        \"activation\": \"gelu\",            # Attivazione diversa\n",
    "        \"norm_type\": \"batchnorm\",\n",
    "        \"num_classes\": 4,\n",
    "        \"learning_rate\": 1e-4,           # Learning rate più basso\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"shallow_leakyrelu_low_dropout\",\n",
    "        \"input_dim\": glove_embedder.vector_size,\n",
    "        \"hidden_dims\": [512],           # Rete più superficiale\n",
    "        \"dropout\": 0.2,\n",
    "        \"activation\": \"leakyrelu\",\n",
    "        \"norm_type\": \"layernorm\",\n",
    "        \"num_classes\": 4,\n",
    "        \"learning_rate\": 1e-3,\n",
    "    }\n",
    "    # Add other configs\n",
    "]\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "MODELS_SAVE_PATH = f\"{ROOT_PATH}/Saved Models\"\n",
    "os.makedirs(MODELS_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Read {len(experiment_configs)} esperimenti.\")\n",
    "print(f\"I modelli migliori verranno salvati in: {MODELS_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ANN with Vector Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDk09uLAji7R"
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1760466324083,
     "user": {
      "displayName": "Francesco d'angolo",
      "userId": "01446161382200832800"
     },
     "user_tz": 300
    },
    "id": "LXi5wpzOjk0x",
    "outputId": "5fd24747-b846-43e6-f92d-661415d9e3f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training and evaluation function defined.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fns, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    loss_weights = {'intensity': 0.2, 'empathy': 0.2, 'polarity': 0.6}\n",
    "\n",
    "    for batch in dataloader:\n",
    "        features = batch['features'].to(device)\n",
    "        labels = {k: v.to(device) for k, v in batch['labels'].items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "\n",
    "        # Calcolo delle loss\n",
    "        loss_intensity = loss_fns['regression'](outputs['intensity'], labels['intensity'])\n",
    "        loss_empathy = loss_fns['regression'](outputs['empathy'], labels['empathy'])\n",
    "        loss_polarity = loss_fns['classification'](outputs['polarity'], labels['polarity'])\n",
    "\n",
    "        # Loss combinata e pesata\n",
    "        combined_loss = (loss_weights['intensity'] * loss_intensity +\n",
    "                         loss_weights['empathy'] * loss_empathy +\n",
    "                         loss_weights['polarity'] * loss_polarity)\n",
    "\n",
    "        # Backward pass e ottimizzazione\n",
    "        optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += combined_loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_performance(model, dataloader, loss_fns, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    loss_weights = {'intensity': 0.2, 'empathy': 0.2, 'polarity': 0.6}\n",
    "\n",
    "    all_intensity_preds, all_intensity_labels = [], []\n",
    "    all_empathy_preds, all_empathy_labels = [], []\n",
    "    all_polarity_preds, all_polarity_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = {k: v.to(device) for k, v in batch['labels'].items()}\n",
    "\n",
    "            outputs = model(features)\n",
    "\n",
    "            loss_intensity = loss_fns['regression'](outputs['intensity'], labels['intensity'])\n",
    "            loss_empathy = loss_fns['regression'](outputs['empathy'], labels['empathy'])\n",
    "            loss_polarity = loss_fns['classification'](outputs['polarity'], labels['polarity'])\n",
    "            combined_loss = (loss_weights['intensity'] * loss_intensity +\n",
    "                             loss_weights['empathy'] * loss_empathy +\n",
    "                             loss_weights['polarity'] * loss_polarity)\n",
    "            total_loss += combined_loss.item()\n",
    "            \n",
    "            all_intensity_preds.append(outputs['intensity'].cpu())\n",
    "            all_intensity_labels.append(labels['intensity'].cpu())\n",
    "            all_empathy_preds.append(outputs['empathy'].cpu())\n",
    "            all_empathy_labels.append(labels['empathy'].cpu())\n",
    "\n",
    "            # Classificazione\n",
    "            polarity_preds = torch.argmax(outputs['polarity'], dim=1)\n",
    "            all_polarity_preds.append(polarity_preds.cpu())\n",
    "            all_polarity_labels.append(labels['polarity'].cpu())\n",
    "\n",
    "    # Concatena i risultati di tutti i batch\n",
    "    all_intensity_preds = torch.cat(all_intensity_preds)\n",
    "    all_intensity_labels = torch.cat(all_intensity_labels)\n",
    "    all_empathy_preds = torch.cat(all_empathy_preds)\n",
    "    all_empathy_labels = torch.cat(all_empathy_labels)\n",
    "    all_polarity_preds = torch.cat(all_polarity_preds)\n",
    "    all_polarity_labels = torch.cat(all_polarity_labels)\n",
    "\n",
    "    # --- Calcolo delle Metriche ---\n",
    "    # MAE per regressione (usando la funzione L1 Loss di PyTorch)\n",
    "    mae_intensity = nn.functional.l1_loss(all_intensity_preds, all_intensity_labels).item()\n",
    "    mae_empathy = nn.functional.l1_loss(all_empathy_preds, all_empathy_labels).item()\n",
    "\n",
    "    # Metriche di classificazione (usando scikit-learn)\n",
    "    accuracy_polarity = accuracy_score(all_polarity_labels, all_polarity_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_polarity_labels, all_polarity_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Ritorna un dizionario con tutte le metriche\n",
    "    metrics = {\n",
    "        \"val_loss\": total_loss / len(dataloader),\n",
    "        \"intensity_mae\": mae_intensity,\n",
    "        \"empathy_mae\": mae_empathy,\n",
    "        \"polarity_accuracy\": accuracy_polarity,\n",
    "        \"polarity_precision\": precision,\n",
    "        \"polarity_recall\": recall,\n",
    "        \"polarity_f1\": f1\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"✅ Training and evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 67032,
     "status": "ok",
     "timestamp": 1760466391118,
     "user": {
      "displayName": "Francesco d'angolo",
      "userId": "01446161382200832800"
     },
     "user_tz": 300
    },
    "id": "lmxA9sX93Uni",
    "outputId": "edfc262f-763b-4c3c-ea7e-c261b12f246a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== START EXPERIMENT: baseline_relu_dropout_0.3 ====================\n",
      "Epoch 1/10 -> Train Loss: 0.8119 | Val Loss: 0.7568 | Polarity F1: 0.6156 | Intensity MAE: 0.5231\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n",
      "Epoch 2/10 -> Train Loss: 0.7127 | Val Loss: 0.7426 | Polarity F1: 0.6191 | Intensity MAE: 0.5334\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n",
      "Epoch 3/10 -> Train Loss: 0.6964 | Val Loss: 0.7388 | Polarity F1: 0.6395 | Intensity MAE: 0.5110\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n",
      "Epoch 4/10 -> Train Loss: 0.6876 | Val Loss: 0.7307 | Polarity F1: 0.6388 | Intensity MAE: 0.5086\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n",
      "Epoch 5/10 -> Train Loss: 0.6751 | Val Loss: 0.7355 | Polarity F1: 0.6562 | Intensity MAE: 0.5461\n",
      "Epoch 6/10 -> Train Loss: 0.6700 | Val Loss: 0.7309 | Polarity F1: 0.6456 | Intensity MAE: 0.5534\n",
      "Epoch 7/10 -> Train Loss: 0.6625 | Val Loss: 0.7252 | Polarity F1: 0.6369 | Intensity MAE: 0.5061\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n",
      "Epoch 8/10 -> Train Loss: 0.6584 | Val Loss: 0.7211 | Polarity F1: 0.6477 | Intensity MAE: 0.5169\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n",
      "Epoch 9/10 -> Train Loss: 0.6528 | Val Loss: 0.7275 | Polarity F1: 0.6262 | Intensity MAE: 0.5011\n",
      "Epoch 10/10 -> Train Loss: 0.6499 | Val Loss: 0.7133 | Polarity F1: 0.6438 | Intensity MAE: 0.5059\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n",
      "\n",
      "NEW BEST MODEL OVERALL: baseline_relu_dropout_0.3 🏆\n",
      "\n",
      "==================== START EXPERIMENT: deep_gelu_dropout_0.5 ====================\n",
      "Epoch 1/10 -> Train Loss: 2.0414 | Val Loss: 1.5461 | Polarity F1: 0.5642 | Intensity MAE: 1.3405\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n",
      "Epoch 2/10 -> Train Loss: 1.1363 | Val Loss: 1.0506 | Polarity F1: 0.5794 | Intensity MAE: 0.8387\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n",
      "Epoch 3/10 -> Train Loss: 0.9277 | Val Loss: 0.8841 | Polarity F1: 0.5734 | Intensity MAE: 0.6466\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n",
      "Epoch 4/10 -> Train Loss: 0.8878 | Val Loss: 0.8642 | Polarity F1: 0.6077 | Intensity MAE: 0.6646\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n",
      "Epoch 5/10 -> Train Loss: 0.8643 | Val Loss: 0.8224 | Polarity F1: 0.6221 | Intensity MAE: 0.6291\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n",
      "Epoch 6/10 -> Train Loss: 0.8367 | Val Loss: 0.8269 | Polarity F1: 0.6234 | Intensity MAE: 0.6210\n",
      "Epoch 7/10 -> Train Loss: 0.8317 | Val Loss: 0.7960 | Polarity F1: 0.6243 | Intensity MAE: 0.6009\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n",
      "Epoch 8/10 -> Train Loss: 0.8114 | Val Loss: 0.8161 | Polarity F1: 0.6251 | Intensity MAE: 0.6126\n",
      "Epoch 9/10 -> Train Loss: 0.7970 | Val Loss: 0.7938 | Polarity F1: 0.6297 | Intensity MAE: 0.6117\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n",
      "Epoch 10/10 -> Train Loss: 0.7919 | Val Loss: 0.7852 | Polarity F1: 0.6343 | Intensity MAE: 0.6123\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/deep_gelu_dropout_0.5_best.pth\n",
      "\n",
      "==================== START EXPERIMENT: shallow_leakyrelu_low_dropout ====================\n",
      "Epoch 1/10 -> Train Loss: 0.7618 | Val Loss: 0.7304 | Polarity F1: 0.6278 | Intensity MAE: 0.5588\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/shallow_leakyrelu_low_dropout_best.pth\n",
      "Epoch 2/10 -> Train Loss: 0.6908 | Val Loss: 0.7337 | Polarity F1: 0.6411 | Intensity MAE: 0.5525\n",
      "Epoch 3/10 -> Train Loss: 0.6709 | Val Loss: 0.7228 | Polarity F1: 0.6363 | Intensity MAE: 0.5504\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/shallow_leakyrelu_low_dropout_best.pth\n",
      "Epoch 4/10 -> Train Loss: 0.6591 | Val Loss: 0.7250 | Polarity F1: 0.6237 | Intensity MAE: 0.5100\n",
      "Epoch 5/10 -> Train Loss: 0.6495 | Val Loss: 0.8202 | Polarity F1: 0.5838 | Intensity MAE: 0.6253\n",
      "Epoch 6/10 -> Train Loss: 0.6420 | Val Loss: 0.7608 | Polarity F1: 0.6266 | Intensity MAE: 0.6075\n",
      "Epoch 7/10 -> Train Loss: 0.6313 | Val Loss: 0.7478 | Polarity F1: 0.6192 | Intensity MAE: 0.5338\n",
      "Epoch 8/10 -> Train Loss: 0.6238 | Val Loss: 0.7202 | Polarity F1: 0.6428 | Intensity MAE: 0.5225\n",
      "  -> New best model for this experiment! Saved in /content/drive/MyDrive/NLPproject1/Saved Models/shallow_leakyrelu_low_dropout_best.pth\n",
      "Epoch 9/10 -> Train Loss: 0.6116 | Val Loss: 0.7333 | Polarity F1: 0.6345 | Intensity MAE: 0.5322\n",
      "Epoch 10/10 -> Train Loss: 0.6020 | Val Loss: 0.7502 | Polarity F1: 0.6217 | Intensity MAE: 0.5369\n",
      "\n",
      "==================== END OF ALL EXPERIMENTS ====================\n"
     ]
    }
   ],
   "source": [
    "# ============= #\n",
    "# EXPERIMENT 1  #\n",
    "# ============= #\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "results = []\n",
    "best_overall_val_loss = float('inf')\n",
    "\n",
    "for config in experiment_configs:\n",
    "    print(f\"\\n{'='*20} START EXPERIMENT: {config['experiment_name']} {'='*20}\")\n",
    "\n",
    "    model = DeepEmpathyNet(config).to(device)\n",
    "\n",
    "    loss_functions = {\n",
    "        'regression': nn.MSELoss(),\n",
    "        'classification': nn.CrossEntropyLoss()\n",
    "    }\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    best_epoch_metrics = {\"val_loss\": float('inf')}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_functions, device)\n",
    "        val_metrics = evaluate_performance(model, eval_loader, loss_functions, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} -> Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['val_loss']:.4f} | \"\n",
    "              f\"Polarity F1: {val_metrics['polarity_f1']:.4f} | Intensity MAE: {val_metrics['intensity_mae']:.4f}\")\n",
    "\n",
    "        # Salva il modello basandoti sulla validation loss\n",
    "        if val_metrics['val_loss'] < best_epoch_metrics['val_loss']:\n",
    "            best_epoch_metrics = val_metrics # Salva tutte le metriche della migliore epoca\n",
    "            model_save_path = os.path.join(MODELS_SAVE_PATH, f\"{config['experiment_name']}_best.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"  -> New best model for this experiment! Saved in {model_save_path}\")\n",
    "\n",
    "    # Aggiungi tutte le metriche della migliore epoca ai risultati finali\n",
    "    final_result = {\"experiment_name\": config['experiment_name'], \"model_path\": model_save_path}\n",
    "    final_result.update(best_epoch_metrics)\n",
    "    results.append(final_result)\n",
    "\n",
    "    if best_epoch_metrics['val_loss'] < best_overall_val_loss:\n",
    "        best_overall_val_loss = best_epoch_metrics['val_loss']\n",
    "        print(f\"\\nNEW BEST MODEL OVERALL: {config['experiment_name']} 🏆\")\n",
    "\n",
    "print(f\"\\n{'='*20} END OF ALL EXPERIMENTS {'='*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGf1qwHhl3bt"
   },
   "source": [
    "### Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1760466391206,
     "user": {
      "displayName": "Francesco d'angolo",
      "userId": "01446161382200832800"
     },
     "user_tz": 300
    },
    "id": "lpxPreeZ5BdJ",
    "outputId": "0c510b0b-6d53-4c6d-9758-23c40862bbfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report salvato con successo!\n",
      "Percorso del file: /content/drive/MyDrive/NLPproject1/Report/experiment_results_20251014.csv\n",
      "\n",
      "--- Report preview ---\n",
      "                      Experiment  Validation Loss  Polarity F1  Polarity Acc.  \\\n",
      "0      baseline_relu_dropout_0.3         0.713343     0.643797       0.654509   \n",
      "1  shallow_leakyrelu_low_dropout         0.720213     0.642761       0.656535   \n",
      "2          deep_gelu_dropout_0.5         0.785186     0.634318       0.651469   \n",
      "\n",
      "   Intensity MAE  Empathy MAE  \n",
      "0       0.505874     0.745510  \n",
      "1       0.522459     0.758093  \n",
      "2       0.612291     0.772994  \n",
      "\n",
      "The best experiment is 'baseline_relu_dropout_0.3'\n",
      "with a validation loss: 0.7133\n",
      "The corresponding model is saved in: /content/drive/MyDrive/NLPproject1/Saved Models/baseline_relu_dropout_0.3_best.pth\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "REPORT_PATH = os.path.join(ROOT_PATH, 'Report')\n",
    "os.makedirs(REPORT_PATH, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "report_filename = f\"experiment_results_{timestamp}.csv\"\n",
    "report_filepath = os.path.join(REPORT_PATH, report_filename)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Seleziona e Rinomina le colonne per una migliore leggibilità\n",
    "display_columns = {\n",
    "    'experiment_name': 'Experiment',\n",
    "    'val_loss': 'Validation Loss',\n",
    "    'polarity_f1': 'Polarity F1',\n",
    "    'polarity_accuracy': 'Polarity Acc.',\n",
    "    'intensity_mae': 'Intensity MAE',\n",
    "    'empathy_mae': 'Empathy MAE'\n",
    "}\n",
    "results_df_display = results_df[list(display_columns.keys())].rename(columns=display_columns)\n",
    "results_df_sorted = results_df_display.sort_values(by=\"Validation Loss\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    results_df_sorted.to_csv(report_filepath, index=False)\n",
    "    print(f\"Report salvato con successo!\")\n",
    "    print(f\"Percorso del file: {report_filepath}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante il salvataggio del report: {e}\")\n",
    "\n",
    "print(\"\\n--- Report preview ---\")\n",
    "saved_report = pd.read_csv(report_filepath)\n",
    "print(saved_report.head())\n",
    "\n",
    "best_experiment_name = results_df_sorted.iloc[0]['Experiment']\n",
    "best_experiment_full_info = next(item for item in results if item[\"experiment_name\"] == best_experiment_name)\n",
    "\n",
    "print(f\"\\nThe best experiment is '{best_experiment_full_info['experiment_name']}'\")\n",
    "print(f\"with a validation loss: {best_experiment_full_info['val_loss']:.4f}\")\n",
    "print(f\"The corresponding model is saved in: {best_experiment_full_info['model_path']}\")\n",
    "\n",
    "\n",
    "# Per caricare il modello migliore in futuro, puoi usare:\n",
    "# model_config = ... (la configurazione del miglior esperimento)\n",
    "# best_model = DeepEmpathyNet(model_config)\n",
    "# best_model.load_state_dict(torch.load(best_experiment['model_path']))\n",
    "# best_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSvn7YY_7yqz"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1760466391729,
     "user": {
      "displayName": "Francesco d'angolo",
      "userId": "01446161382200832800"
     },
     "user_tz": 300
    },
    "id": "e9eh5EuX70JZ",
    "outputId": "5173dba9-def0-4ec8-9017-6ea4f53dde5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start inference on test set ---\n",
      "Best model: 'baseline_relu_dropout_0.3'\n",
      "Generazione degli embedding per 2311 campioni di test...\n",
      "Predictions running...\n",
      "Predictions completed.\n",
      "\n",
      "✅ File di submission creato con successo!\n",
      "Percorso del file: /content/drive/MyDrive/NLPproject1/Report/ann_report.csv\n",
      "\n",
      "--- Anteprima del file di submission ---\n",
      "   id   Emotion  EmotionalPolarity   Empathy\n",
      "0   1  2.195595                  2  2.051561\n",
      "1   2  2.463433                  2  2.298273\n",
      "2   3  2.412533                  2  2.224228\n",
      "3   4  2.161638                  2  2.134687\n",
      "4   5  2.682936                  2  2.594292\n"
     ]
    }
   ],
   "source": [
    "from dataset import InferenceDataset\n",
    "\n",
    "TEST_CSV_PATH = f\"{DATA_PATH}/trac2_CONVT_test.csv\"\n",
    "SUBMISSION_PATH = os.path.join(REPORT_PATH, 'ann_report.csv')\n",
    "ID_COLUMN_NAME = 'id'       # Verifica il nome della colonna ID nel file di test\n",
    "TEXT_COLUMN_NAME = 'text'   # Verifica il nome della colonna di testo\n",
    "\n",
    "print(\"--- Start inference on test set ---\")\n",
    "\n",
    "# Recuperiamo la configurazione e il percorso del miglior esperimento\n",
    "best_experiment_name = results_df_sorted.iloc[0]['Experiment']\n",
    "best_experiment_info = next(item for item in experiment_configs if item[\"experiment_name\"] == best_experiment_name)\n",
    "best_model_path = os.path.join(MODELS_SAVE_PATH, f\"{best_experiment_name}_best.pth\")\n",
    "\n",
    "print(f\"Best model: '{best_experiment_name}'\")\n",
    "model = DeepEmpathyNet(best_experiment_info).to(device)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "test_dataset = InferenceDataset(\n",
    "    csv_path=TEST_CSV_PATH,\n",
    "    embedder=glove_embedder,\n",
    "    id_column=ID_COLUMN_NAME,\n",
    "    text_column=TEXT_COLUMN_NAME\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # shuffle=False è importante!\n",
    "\n",
    "\n",
    "all_ids = []\n",
    "all_emotion_preds = []\n",
    "all_empathy_preds = []\n",
    "all_polarity_preds = []\n",
    "\n",
    "print(\"Predictions running...\")\n",
    "with torch.no_grad(): # Disabilita il calcolo dei gradienti per velocizzare\n",
    "    for batch in test_loader:\n",
    "        ids = batch['id']\n",
    "        features = batch['features'].to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        emotion_preds = outputs['intensity'].squeeze().cpu().numpy()\n",
    "        empathy_preds = outputs['empathy'].squeeze().cpu().numpy()\n",
    "        polarity_preds = torch.argmax(outputs['polarity'], dim=1).cpu().numpy()\n",
    "\n",
    "        all_ids.extend(ids.numpy())\n",
    "        all_emotion_preds.extend(emotion_preds)\n",
    "        all_empathy_preds.extend(empathy_preds)\n",
    "        all_polarity_preds.extend(polarity_preds)\n",
    "\n",
    "print(\"Predictions completed.\")\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': all_ids,\n",
    "    'Emotion': all_emotion_preds,\n",
    "    'EmotionalPolarity': all_polarity_preds,\n",
    "    'Empathy': all_empathy_preds\n",
    "})\n",
    "\n",
    "submission_df['EmotionalPolarity'] = submission_df['EmotionalPolarity'].astype(int)\n",
    "\n",
    "\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(f\"\\n✅ File di submission creato con successo!\")\n",
    "print(f\"Percorso del file: {SUBMISSION_PATH}\")\n",
    "\n",
    "print(\"\\n--- Anteprima del file di submission ---\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
