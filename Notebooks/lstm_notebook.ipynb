{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08b8c69",
   "metadata": {},
   "source": [
    "# LSTM Model - Emotion and Empathy Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d3d9f",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# This cell works only on colab  #\n",
    "##################################\n",
    "import sys\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "DRIVE_NAME = \"NLPproject1\"\n",
    "ROOT_PATH = os.path.join('/content/drive/MyDrive/', DRIVE_NAME)\n",
    "SCRIPTS_PATH = f'{ROOT_PATH}/Scripts'\n",
    "DATA_PATH = f'{ROOT_PATH}/Dataset'\n",
    "\n",
    "if SCRIPTS_PATH not in sys.path:\n",
    "    sys.path.append(SCRIPTS_PATH)\n",
    "\n",
    "from preprocessing import GloveSequenceEmbedder\n",
    "from dataset import RNNEmpathyDataset, RNNInferenceDataset\n",
    "from lstm_model import LSTMEmpathyNet\n",
    "\n",
    "print(\"Setup completed\")\n",
    "print(f\"Root Path: {ROOT_PATH}\")\n",
    "print(f\"Scripts Path: {SCRIPTS_PATH}\")\n",
    "print(f\"Data Path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70788ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# This cell works for VS Code    #\n",
    "##################################\n",
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "SCRIPTS_PATH = os.path.join(ROOT_PATH, 'Scripts')\n",
    "DATA_PATH = os.path.join(ROOT_PATH, 'Dataset')\n",
    "\n",
    "if SCRIPTS_PATH not in sys.path:\n",
    "    sys.path.append(SCRIPTS_PATH)\n",
    "\n",
    "from preprocessing import GloveSequenceEmbedder\n",
    "from dataset import RNNEmpathyDataset, RNNInferenceDataset\n",
    "from lstm_model import LSTMEmpathyNet\n",
    "\n",
    "print(\"Setup completed\")\n",
    "print(f\"Root Path: {ROOT_PATH}\")\n",
    "print(f\"Scripts Path: {SCRIPTS_PATH}\")\n",
    "print(f\"Data Path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f2d6f",
   "metadata": {},
   "source": [
    "## Initialize GloVe Sequence Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ce988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing GloVe Sequence Embedder...\")\n",
    "sequence_embedder = GloveSequenceEmbedder(\n",
    "    model_name=\"glove-wiki-gigaword-100\",\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "print(f\"Sequence Embedder initialized\")\n",
    "print(f\"Vector size: {sequence_embedder.vector_size}\")\n",
    "print(f\"Max length: {sequence_embedder.max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73585e84",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_loss = nn.MSELoss()\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_weights = {\n",
    "    'intensity': 0.2,\n",
    "    'empathy': 0.2,\n",
    "    'polarity': 0.6\n",
    "}\n",
    "\n",
    "loss_functions = {\n",
    "    'regression': regression_loss,\n",
    "    'classification': classification_loss\n",
    "}\n",
    "\n",
    "print(\"Loss functions configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44d76f",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dba81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = f\"{DATA_PATH}/trac2_CONVT_train.csv\"\n",
    "eval_csv_path = f\"{DATA_PATH}/trac2_CONVT_dev.csv\"\n",
    "\n",
    "print(\"Creating LSTM datasets...\")\n",
    "lstm_train_dataset = RNNEmpathyDataset(csv_path=train_csv_path, embedder=sequence_embedder)\n",
    "lstm_eval_dataset = RNNEmpathyDataset(csv_path=eval_csv_path, embedder=sequence_embedder)\n",
    "\n",
    "LSTM_BATCH_SIZE = 32\n",
    "lstm_train_loader = DataLoader(lstm_train_dataset, batch_size=LSTM_BATCH_SIZE, shuffle=True)\n",
    "lstm_eval_loader = DataLoader(lstm_eval_dataset, batch_size=LSTM_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(lstm_train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(lstm_eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154cc08",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_experiment_configs = [\n",
    "    {\n",
    "        \"experiment_name\": \"lstm_baseline_h128_d0.3\",\n",
    "        \"input_dim\": sequence_embedder.vector_size,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"dropout\": 0.3,\n",
    "        \"num_classes\": 4,\n",
    "        \"learning_rate\": 1e-3,\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"lstm_deep_h256_d0.5\",\n",
    "        \"input_dim\": sequence_embedder.vector_size,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"num_layers\": 3,\n",
    "        \"bidirectional\": True,\n",
    "        \"dropout\": 0.5,\n",
    "        \"num_classes\": 4,\n",
    "        \"learning_rate\": 5e-4,\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"lstm_unidirectional_h128_d0.2\",\n",
    "        \"input_dim\": sequence_embedder.vector_size,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"bidirectional\": False,\n",
    "        \"dropout\": 0.2,\n",
    "        \"num_classes\": 4,\n",
    "        \"learning_rate\": 1e-3,\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"lstm_wide_h512_d0.4\",\n",
    "        \"input_dim\": sequence_embedder.vector_size,\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_layers\": 2,\n",
    "        \"bidirectional\": True,\n",
    "        \"dropout\": 0.4,\n",
    "        \"num_classes\": 4,\n",
    "        \"learning_rate\": 1e-4,\n",
    "    }\n",
    "]\n",
    "\n",
    "NUM_LSTM_EPOCHS = 15\n",
    "LSTM_MODELS_SAVE_PATH = f\"{ROOT_PATH}/Saved Models/LSTM\"\n",
    "os.makedirs(LSTM_MODELS_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Configured {len(lstm_experiment_configs)} LSTM experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2b663",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_one_epoch(model, dataloader, optimizer, loss_fns, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    loss_weights = {'intensity': 0.2, 'empathy': 0.2, 'polarity': 0.6}\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        features = batch['features'].to(device)\n",
    "        lengths = batch['lengths'].to(device)\n",
    "        labels = {k: v.to(device) for k, v in batch['labels'].items()}\n",
    "        \n",
    "        outputs = model(features, lengths)\n",
    "        \n",
    "        loss_intensity = loss_fns['regression'](outputs['intensity'], labels['intensity'])\n",
    "        loss_empathy = loss_fns['regression'](outputs['empathy'], labels['empathy'])\n",
    "        loss_polarity = loss_fns['classification'](outputs['polarity'], labels['polarity'])\n",
    "        \n",
    "        combined_loss = (loss_weights['intensity'] * loss_intensity +\n",
    "                        loss_weights['empathy'] * loss_empathy +\n",
    "                        loss_weights['polarity'] * loss_polarity)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += combined_loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate_lstm_performance(model, dataloader, loss_fns, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    loss_weights = {'intensity': 0.2, 'empathy': 0.2, 'polarity': 0.6}\n",
    "    \n",
    "    all_intensity_preds, all_intensity_labels = [], []\n",
    "    all_empathy_preds, all_empathy_labels = [], []\n",
    "    all_polarity_preds, all_polarity_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features = batch['features'].to(device)\n",
    "            lengths = batch['lengths'].to(device)\n",
    "            labels = {k: v.to(device) for k, v in batch['labels'].items()}\n",
    "            \n",
    "            outputs = model(features, lengths)\n",
    "            \n",
    "            loss_intensity = loss_fns['regression'](outputs['intensity'], labels['intensity'])\n",
    "            loss_empathy = loss_fns['regression'](outputs['empathy'], labels['empathy'])\n",
    "            loss_polarity = loss_fns['classification'](outputs['polarity'], labels['polarity'])\n",
    "            combined_loss = (loss_weights['intensity'] * loss_intensity +\n",
    "                            loss_weights['empathy'] * loss_empathy +\n",
    "                            loss_weights['polarity'] * loss_polarity)\n",
    "            total_loss += combined_loss.item()\n",
    "            \n",
    "            all_intensity_preds.append(outputs['intensity'].cpu())\n",
    "            all_intensity_labels.append(labels['intensity'].cpu())\n",
    "            all_empathy_preds.append(outputs['empathy'].cpu())\n",
    "            all_empathy_labels.append(labels['empathy'].cpu())\n",
    "            \n",
    "            polarity_preds = torch.argmax(outputs['polarity'], dim=1)\n",
    "            all_polarity_preds.append(polarity_preds.cpu())\n",
    "            all_polarity_labels.append(labels['polarity'].cpu())\n",
    "    \n",
    "    all_intensity_preds = torch.cat(all_intensity_preds)\n",
    "    all_intensity_labels = torch.cat(all_intensity_labels)\n",
    "    all_empathy_preds = torch.cat(all_empathy_preds)\n",
    "    all_empathy_labels = torch.cat(all_empathy_labels)\n",
    "    all_polarity_preds = torch.cat(all_polarity_preds)\n",
    "    all_polarity_labels = torch.cat(all_polarity_labels)\n",
    "    \n",
    "    mae_intensity = nn.functional.l1_loss(all_intensity_preds, all_intensity_labels).item()\n",
    "    mae_empathy = nn.functional.l1_loss(all_empathy_preds, all_empathy_labels).item()\n",
    "    \n",
    "    accuracy_polarity = accuracy_score(all_polarity_labels, all_polarity_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_polarity_labels, all_polarity_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        \"val_loss\": total_loss / len(dataloader),\n",
    "        \"intensity_mae\": mae_intensity,\n",
    "        \"empathy_mae\": mae_empathy,\n",
    "        \"polarity_accuracy\": accuracy_polarity,\n",
    "        \"polarity_precision\": precision,\n",
    "        \"polarity_recall\": recall,\n",
    "        \"polarity_f1\": f1\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b59c24",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "lstm_results = []\n",
    "best_overall_val_loss = float('inf')\n",
    "\n",
    "for config in lstm_experiment_configs:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Experiment: {config['experiment_name']}\")\n",
    "    \n",
    "    model = LSTMEmpathyNet(config).to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {total_params:,}\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_epoch_metrics = {\"val_loss\": float('inf')}\n",
    "    patience_counter = 0\n",
    "    early_stopping_patience = 5\n",
    "    \n",
    "    for epoch in range(NUM_LSTM_EPOCHS):\n",
    "        train_loss = train_lstm_one_epoch(model, lstm_train_loader, optimizer, loss_functions, device)\n",
    "        val_metrics = evaluate_lstm_performance(model, lstm_eval_loader, loss_functions, device)\n",
    "        \n",
    "        scheduler.step(val_metrics['val_loss'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_LSTM_EPOCHS} -> Train: {train_loss:.4f} | Val: {val_metrics['val_loss']:.4f} | F1: {val_metrics['polarity_f1']:.4f}\")\n",
    "        \n",
    "        if val_metrics['val_loss'] < best_epoch_metrics['val_loss']:\n",
    "            best_epoch_metrics = val_metrics\n",
    "            model_save_path = os.path.join(LSTM_MODELS_SAVE_PATH, f\"{config['experiment_name']}_best.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    final_result = {\n",
    "        \"experiment_name\": config['experiment_name'],\n",
    "        \"model_path\": model_save_path,\n",
    "        \"hidden_dim\": config['hidden_dim'],\n",
    "        \"num_layers\": config['num_layers'],\n",
    "        \"bidirectional\": config['bidirectional'],\n",
    "        \"dropout\": config['dropout'],\n",
    "        \"learning_rate\": config['learning_rate'],\n",
    "        \"total_params\": total_params\n",
    "    }\n",
    "    final_result.update(best_epoch_metrics)\n",
    "    lstm_results.append(final_result)\n",
    "    \n",
    "    if best_epoch_metrics['val_loss'] < best_overall_val_loss:\n",
    "        best_overall_val_loss = best_epoch_metrics['val_loss']\n",
    "        print(f\"New best model: {config['experiment_name']}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best validation loss: {best_overall_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419cc1c8",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f331d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_results_df = pd.DataFrame(lstm_results)\n",
    "\n",
    "lstm_display_columns = {\n",
    "    'experiment_name': 'Experiment',\n",
    "    'hidden_dim': 'Hidden',\n",
    "    'num_layers': 'Layers',\n",
    "    'bidirectional': 'Bidir',\n",
    "    'dropout': 'Dropout',\n",
    "    'val_loss': 'Val Loss',\n",
    "    'intensity_mae': 'Int MAE',\n",
    "    'empathy_mae': 'Emp MAE',\n",
    "    'polarity_f1': 'Pol F1'\n",
    "}\n",
    "\n",
    "lstm_results_display = lstm_results_df[list(lstm_display_columns.keys())].rename(columns=lstm_display_columns)\n",
    "lstm_results_sorted = lstm_results_display.sort_values(by=\"Val Loss\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "REPORT_PATH = os.path.join(ROOT_PATH, 'Report')\n",
    "os.makedirs(REPORT_PATH, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "lstm_report_filepath = os.path.join(REPORT_PATH, f\"lstm_results_{timestamp}.csv\")\n",
    "lstm_results_sorted.to_csv(lstm_report_filepath, index=False)\n",
    "\n",
    "print(\"LSTM Experiment Results:\")\n",
    "print(lstm_results_sorted.to_string(index=False))\n",
    "\n",
    "best_experiment_name = lstm_results_sorted.iloc[0]['Experiment']\n",
    "best_experiment_full_info = next(item for item in lstm_results if item[\"experiment_name\"] == best_experiment_name)\n",
    "\n",
    "print(f\"\\nBest model: {best_experiment_full_info['experiment_name']}\")\n",
    "print(f\"Val Loss: {best_experiment_full_info['val_loss']:.4f}\")\n",
    "print(f\"Intensity MAE: {best_experiment_full_info['intensity_mae']:.4f}\")\n",
    "print(f\"Empathy MAE: {best_experiment_full_info['empathy_mae']:.4f}\")\n",
    "print(f\"Polarity F1: {best_experiment_full_info['polarity_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800cc08f",
   "metadata": {},
   "source": [
    "## Evaluation on Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_config = next(config for config in lstm_experiment_configs \n",
    "                         if config[\"experiment_name\"] == best_experiment_name)\n",
    "best_lstm_model = LSTMEmpathyNet(best_model_config).to(device)\n",
    "best_lstm_model.load_state_dict(torch.load(best_experiment_full_info['model_path']))\n",
    "best_lstm_model.eval()\n",
    "\n",
    "all_intensity_preds, all_intensity_labels = [], []\n",
    "all_empathy_preds, all_empathy_labels = [], []\n",
    "all_polarity_preds, all_polarity_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in lstm_eval_loader:\n",
    "        features = batch['features'].to(device)\n",
    "        lengths = batch['lengths'].to(device)\n",
    "        labels = {k: v.to(device) for k, v in batch['labels'].items()}\n",
    "        \n",
    "        outputs = best_lstm_model(features, lengths)\n",
    "        \n",
    "        all_intensity_preds.append(outputs['intensity'].cpu())\n",
    "        all_intensity_labels.append(labels['intensity'].cpu())\n",
    "        all_empathy_preds.append(outputs['empathy'].cpu())\n",
    "        all_empathy_labels.append(labels['empathy'].cpu())\n",
    "        \n",
    "        polarity_preds = torch.argmax(outputs['polarity'], dim=1)\n",
    "        all_polarity_preds.append(polarity_preds.cpu())\n",
    "        all_polarity_labels.append(labels['polarity'].cpu())\n",
    "\n",
    "all_intensity_preds = torch.cat(all_intensity_preds).numpy()\n",
    "all_intensity_labels = torch.cat(all_intensity_labels).numpy()\n",
    "all_empathy_preds = torch.cat(all_empathy_preds).numpy()\n",
    "all_empathy_labels = torch.cat(all_empathy_labels).numpy()\n",
    "all_polarity_preds = torch.cat(all_polarity_preds).numpy()\n",
    "all_polarity_labels = torch.cat(all_polarity_labels).numpy()\n",
    "\n",
    "print(\"REGRESSION METRICS\")\n",
    "print(f\"\\nEmotion Intensity:\")\n",
    "print(f\"  MAE: {mean_absolute_error(all_intensity_labels, all_intensity_preds):.4f}\")\n",
    "print(f\"  MSE: {mean_squared_error(all_intensity_labels, all_intensity_preds):.4f}\")\n",
    "print(f\"  R²: {r2_score(all_intensity_labels, all_intensity_preds):.4f}\")\n",
    "\n",
    "print(f\"\\nEmpathy:\")\n",
    "print(f\"  MAE: {mean_absolute_error(all_empathy_labels, all_empathy_preds):.4f}\")\n",
    "print(f\"  MSE: {mean_squared_error(all_empathy_labels, all_empathy_preds):.4f}\")\n",
    "print(f\"  R²: {r2_score(all_empathy_labels, all_empathy_preds):.4f}\")\n",
    "\n",
    "print(f\"\\nCLASSIFICATION METRICS\")\n",
    "print(f\"\\nEmotional Polarity:\")\n",
    "polarity_accuracy = accuracy_score(all_polarity_labels, all_polarity_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_polarity_labels, all_polarity_preds, average='weighted', zero_division=0)\n",
    "print(f\"  Accuracy: {polarity_accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_polarity_labels, all_polarity_preds)\n",
    "for i in range(4):\n",
    "    print(f\"  {cm[i]}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(all_polarity_labels, all_polarity_preds, \n",
    "                           target_names=[f'Class {i}' for i in range(4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ee16e",
   "metadata": {},
   "source": [
    "## Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CSV_PATH = f\"{DATA_PATH}/trac2_CONVT_test.csv\"\n",
    "LSTM_SUBMISSION_PATH = os.path.join(REPORT_PATH, 'lstm_report.csv')\n",
    "\n",
    "print(\"Running predictions on test set...\")\n",
    "\n",
    "test_dataset = RNNInferenceDataset(\n",
    "    csv_path=TEST_CSV_PATH,\n",
    "    embedder=sequence_embedder,\n",
    "    id_column='id',\n",
    "    text_column='text'\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=LSTM_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "all_ids = []\n",
    "all_emotion_preds = []\n",
    "all_empathy_preds = []\n",
    "all_polarity_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ids = batch['id']\n",
    "        features = batch['features'].to(device)\n",
    "        lengths = batch['lengths'].to(device)\n",
    "        \n",
    "        outputs = best_lstm_model(features, lengths)\n",
    "        \n",
    "        emotion_preds = outputs['intensity'].squeeze().cpu().numpy()\n",
    "        empathy_preds = outputs['empathy'].squeeze().cpu().numpy()\n",
    "        polarity_preds = torch.argmax(outputs['polarity'], dim=1).cpu().numpy()\n",
    "        \n",
    "        all_ids.extend(ids.numpy())\n",
    "        all_emotion_preds.extend(emotion_preds)\n",
    "        all_empathy_preds.extend(empathy_preds)\n",
    "        all_polarity_preds.extend(polarity_preds)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': all_ids,\n",
    "    'Emotion': all_emotion_preds,\n",
    "    'EmotionalPolarity': all_polarity_preds,\n",
    "    'Empathy': all_empathy_preds\n",
    "})\n",
    "\n",
    "submission_df['EmotionalPolarity'] = submission_df['EmotionalPolarity'].astype(int)\n",
    "submission_df.to_csv(LSTM_SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(f\"Submission file created: {LSTM_SUBMISSION_PATH}\")\n",
    "print(f\"\\nPreview:\")\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
