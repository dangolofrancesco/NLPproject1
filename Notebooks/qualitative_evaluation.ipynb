{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790ceaba",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation: Comparing ANN, LSTM, and LLM Models\n",
    "\n",
    "This notebook performs a detailed qualitative analysis of model predictions on 5 conversations from the development dataset.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "1. Emotion Polarity: Correct vs. Incorrect predictions\n",
    "2. Emotion Intensity: Close vs. Far off predictions\n",
    "3. Empathy: Close vs. Far off predictions\n",
    "4. Model comparison per task\n",
    "5. Overall best model across all tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa4090",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f0669f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc9e34",
   "metadata": {},
   "source": [
    "## 2. Load All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130b32ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development dataset shape: (987, 13)\n",
      "Columns: ['id', 'article_id', 'conversation_id', 'turn_id', 'speaker_id', 'text', 'person_id', 'person_id_1', 'person_id_2', 'Emotion', 'EmotionalPolarity', 'Empathy', 'SelfDisclosure']\n",
      "\n",
      "First few rows:\n",
      "   id  article_id  conversation_id  turn_id  speaker_id  \\\n",
      "0   1           8               68        1          38   \n",
      "1   2           8               68        2          75   \n",
      "2   3           8               68        3          38   \n",
      "3   4           8               68        4          75   \n",
      "4   5           8               68        5          38   \n",
      "\n",
      "                                                text  person_id  person_id_1  \\\n",
      "0                                 Hello how are you?         38           38   \n",
      "1                          Hello! Fine. How are you?         75           38   \n",
      "2                        I'm good thanks for asking!         38           38   \n",
      "3  That's good, so I guess we are supposed to dis...         75           38   \n",
      "4       That's a very scary thought to have honestly         38           38   \n",
      "\n",
      "   person_id_2  Emotion  EmotionalPolarity  Empathy  SelfDisclosure  \n",
      "0           75      1.0                1.0      1.0             NaN  \n",
      "1           75      2.0                1.0      2.0             NaN  \n",
      "2           75      2.0                1.0      2.0             NaN  \n",
      "3           75      2.0                1.0      3.0             NaN  \n",
      "4           75      3.0                2.0      4.0             NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load development dataset with error handling\n",
    "dev_df = pd.read_csv('../Dataset/trac2_CONVT_dev.csv', on_bad_lines='skip')\n",
    "print(f\"Development dataset shape: {dev_df.shape}\")\n",
    "print(f\"Columns: {dev_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(dev_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52defb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANN predictions shape: (2311, 4)\n",
      "Columns: ['id', 'Emotion', 'EmotionalPolarity', 'Empathy']\n",
      "   id   Emotion  EmotionalPolarity   Empathy\n",
      "0   1  2.195595                  2  2.051561\n",
      "1   2  2.463433                  2  2.298273\n",
      "2   3  2.412533                  2  2.224228\n",
      "3   4  2.161637                  2  2.134687\n",
      "4   5  2.682936                  2  2.594292\n"
     ]
    }
   ],
   "source": [
    "# Load ANN predictions\n",
    "ann_df = pd.read_csv('../Report/ann_report.csv')\n",
    "print(f\"\\nANN predictions shape: {ann_df.shape}\")\n",
    "print(f\"Columns: {ann_df.columns.tolist()}\")\n",
    "print(ann_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42381a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM predictions shape: (2311, 4)\n",
      "Columns: ['id', 'Emotion', 'EmotionalPolarity', 'Empathy']\n",
      "   id   Emotion  EmotionalPolarity   Empathy\n",
      "0   1  2.175447                  1  1.911928\n",
      "1   2  2.437121                  2  2.494642\n",
      "2   3  2.269280                  2  2.163032\n",
      "3   4  2.258986                  2  2.234717\n",
      "4   5  2.344783                  2  2.296820\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM predictions\n",
    "lstm_df = pd.read_csv('../Report/lstm_report.csv')\n",
    "print(f\"\\nLSTM predictions shape: {lstm_df.shape}\")\n",
    "print(f\"Columns: {lstm_df.columns.tolist()}\")\n",
    "print(lstm_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d21af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Model: llama-3.3-70b-versatile\n",
      "Timestamp: 2025-10-27 22:42:35\n",
      "Number of conversations: 5\n",
      "\n",
      "Conversation details:\n",
      "  Conv 1: Article 35, 22 turns\n",
      "  Conv 2: Article 35, 21 turns\n",
      "  Conv 3: Article 35, 25 turns\n",
      "  Conv 4: Article 35, 31 turns\n",
      "  Conv 5: Article 35, 20 turns\n"
     ]
    }
   ],
   "source": [
    "# Load LLM predictions\n",
    "with open('../Report/LLM_output.json', 'r') as f:\n",
    "    llm_data = json.load(f)\n",
    "\n",
    "print(f\"\\nLLM Model: {llm_data['model']}\")\n",
    "print(f\"Timestamp: {llm_data['timestamp']}\")\n",
    "print(f\"Number of conversations: {len(llm_data['conversations'])}\")\n",
    "print(f\"\\nConversation details:\")\n",
    "for conv in llm_data['conversations']:\n",
    "    print(f\"  Conv {conv['id']}: Article {conv['article_id']}, {conv['num_turns']} turns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa897a",
   "metadata": {},
   "source": [
    "## 3. Extract First 5 Turns from Each of 5 Conversations\n",
    "\n",
    "We'll analyze conversations 1-5 from article 35, focusing on the first 5 turns of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f522d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected conversations from dev dataset:\n",
      "   article_id  conversation_id  num_turns\n",
      "0           8               68         30\n",
      "1           9               72         30\n",
      "2           9               74         30\n",
      "3          10               80         30\n",
      "4          10               96         30\n",
      "\\nProcessing Article 8, Conversation 68...\n",
      "  Found 5 turns\n",
      "\\nProcessing Article 9, Conversation 72...\n",
      "  Found 5 turns\n",
      "\\nProcessing Article 9, Conversation 74...\n",
      "  Found 5 turns\n",
      "\\nProcessing Article 10, Conversation 80...\n",
      "  Found 5 turns\n",
      "\\nProcessing Article 10, Conversation 96...\n",
      "  Found 5 turns\n",
      "\\n================================================================================\n",
      "Total turns collected for analysis: 25\n",
      "Total conversations: 5\n",
      "================================================================================\n",
      "\\nFirst few rows:\n",
      "   conversation_id  turn_num  \\\n",
      "0               68         1   \n",
      "1               68         2   \n",
      "2               68         3   \n",
      "3               68         4   \n",
      "4               68         5   \n",
      "5               72         1   \n",
      "6               72         2   \n",
      "7               72         3   \n",
      "8               72         4   \n",
      "9               72         5   \n",
      "\n",
      "                                                text  gold_intensity  \\\n",
      "0                                 Hello how are you?             1.0   \n",
      "1                          Hello! Fine. How are you?             2.0   \n",
      "2                        I'm good thanks for asking!             2.0   \n",
      "3  That's good, so I guess we are supposed to dis...             2.0   \n",
      "4       That's a very scary thought to have honestly             3.0   \n",
      "5                                hello anyone there?             1.0   \n",
      "6                                                 hi             1.0   \n",
      "7                     did we read the same article?              1.0   \n",
      "8                      Was yours about an explosion?             1.0   \n",
      "9     yeah, some guy stopped to sell fuel then boom!             2.0   \n",
      "\n",
      "   ann_intensity  lstm_intensity  \n",
      "0       2.195595        2.175447  \n",
      "1       2.463433        2.437121  \n",
      "2       2.412533        2.269280  \n",
      "3       2.161637        2.258986  \n",
      "4       2.682936        2.344783  \n",
      "5       1.616909        1.854176  \n",
      "6       2.396988        2.350312  \n",
      "7       1.872122        1.892616  \n",
      "8       1.594821        1.870156  \n",
      "9       2.070308        2.208659  \n"
     ]
    }
   ],
   "source": [
    "# Configuration: Use conversations from the dev dataset\n",
    "# Note: The LLM was evaluated on different conversations (article 35) \n",
    "# not present in the dev set, so we'll analyze ANN and LSTM on dev set conversations\n",
    "# and discuss LLM separately\n",
    "\n",
    "# Select first 5 conversations from dev dataset\n",
    "conversations_in_dev = dev_df.groupby(['article_id', 'conversation_id']).size().reset_index(name='num_turns')\n",
    "conversations_in_dev = conversations_in_dev[conversations_in_dev['num_turns'] >= 5]  # Ensure enough turns\n",
    "\n",
    "# Take first 5 conversations\n",
    "selected_convs = conversations_in_dev.head(5)\n",
    "print(\"Selected conversations from dev dataset:\")\n",
    "print(selected_convs)\n",
    "\n",
    "TURNS_PER_CONVERSATION = 5\n",
    "\n",
    "# Collect data for analysis\n",
    "analysis_data = []\n",
    "\n",
    "for _, conv_info in selected_convs.iterrows():\n",
    "    article_id = conv_info['article_id']\n",
    "    conv_id = conv_info['conversation_id']\n",
    "    \n",
    "    print(f\"\\\\nProcessing Article {article_id}, Conversation {conv_id}...\")\n",
    "    \n",
    "    # Get first 5 turns from this conversation\n",
    "    conv_data = dev_df[\n",
    "        (dev_df['article_id'] == article_id) & \n",
    "        (dev_df['conversation_id'] == conv_id)\n",
    "    ].head(TURNS_PER_CONVERSATION)\n",
    "    \n",
    "    print(f\"  Found {len(conv_data)} turns\")\n",
    "    \n",
    "    # For each turn\n",
    "    for turn_num, (_, row) in enumerate(conv_data.iterrows(), 1):\n",
    "        turn_id = row['id']\n",
    "        \n",
    "        # Get ANN and LSTM predictions\n",
    "        ann_pred = ann_df[ann_df['id'] == turn_id]\n",
    "        lstm_pred = lstm_df[lstm_df['id'] == turn_id]\n",
    "        \n",
    "        if len(ann_pred) == 0 or len(lstm_pred) == 0:\n",
    "            print(f\"  Warning: Missing predictions for turn ID {turn_id}\")\n",
    "            continue\n",
    "        \n",
    "        ann_pred = ann_pred.iloc[0]\n",
    "        lstm_pred = lstm_pred.iloc[0]\n",
    "        \n",
    "        # Store all data\n",
    "        turn_data = {\n",
    "            'conversation_id': conv_id,\n",
    "            'article_id': article_id,\n",
    "            'turn_num': turn_num,\n",
    "            'turn_id': turn_id,\n",
    "            'text': row['text'],\n",
    "            # Ground truth\n",
    "            'gold_intensity': row['Emotion'],\n",
    "            'gold_empathy': row['Empathy'],\n",
    "            'gold_polarity': row['EmotionalPolarity'],\n",
    "            # ANN predictions\n",
    "            'ann_intensity': ann_pred['Emotion'],\n",
    "            'ann_empathy': ann_pred['Empathy'],\n",
    "            'ann_polarity': ann_pred['EmotionalPolarity'],\n",
    "            # LSTM predictions  \n",
    "            'lstm_intensity': lstm_pred['Emotion'],\n",
    "            'lstm_empathy': lstm_pred['Empathy'],\n",
    "            'lstm_polarity': lstm_pred['EmotionalPolarity']\n",
    "        }\n",
    "        \n",
    "        analysis_data.append(turn_data)\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "results_df = pd.DataFrame(analysis_data)\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(f\"Total turns collected for analysis: {len(results_df)}\")\n",
    "print(f\"Total conversations: {results_df['conversation_id'].nunique()}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\\\nFirst few rows:\")\n",
    "print(results_df[['conversation_id', 'turn_num', 'text', 'gold_intensity', 'ann_intensity', 'lstm_intensity']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d0cc3",
   "metadata": {},
   "source": [
    "## 4. Display Detailed Predictions for Each Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca3de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n====================================================================================================\n",
      "CONVERSATION 68 (Article 8)\n",
      "====================================================================================================\n",
      "\\n--- Turn 1 (ID: 1) ---\n",
      "Text: Hello how are you?\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         1.0       2.20       2.18\n",
      "Empathy                   1.0       2.05       1.91\n",
      "Polarity                    1          2          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 2 (ID: 2) ---\n",
      "Text: Hello! Fine. How are you?\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         2.0       2.46       2.44\n",
      "Empathy                   2.0       2.30       2.49\n",
      "Polarity                    1          2          2\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 3 (ID: 3) ---\n",
      "Text: I'm good thanks for asking!\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         2.0       2.41       2.27\n",
      "Empathy                   2.0       2.22       2.16\n",
      "Polarity                    1          2          2\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 4 (ID: 4) ---\n",
      "Text: That's good, so I guess we are supposed to discuss the article. I wonder, if my family or I were in ...\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         2.0       2.16       2.26\n",
      "Empathy                   3.0       2.13       2.23\n",
      "Polarity                    1          2          2\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 5 (ID: 5) ---\n",
      "Text: That's a very scary thought to have honestly\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         3.0       2.68       2.34\n",
      "Empathy                   4.0       2.59       2.30\n",
      "Polarity                    2          2          2\n",
      "--------------------------------------------------------------------------------\n",
      "\\n\n",
      "\\n====================================================================================================\n",
      "CONVERSATION 72 (Article 9)\n",
      "====================================================================================================\n",
      "\\n--- Turn 1 (ID: 31) ---\n",
      "Text: hello anyone there?\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         1.0       1.62       1.85\n",
      "Empathy                   1.0       1.64       1.79\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 2 (ID: 32) ---\n",
      "Text: hi\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         1.0       2.40       2.35\n",
      "Empathy                   1.0       2.44       2.39\n",
      "Polarity                    1          2          2\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 3 (ID: 33) ---\n",
      "Text: did we read the same article? \n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         1.0       1.87       1.89\n",
      "Empathy                   1.0       1.92       1.83\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 4 (ID: 34) ---\n",
      "Text: Was yours about an explosion?\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         1.0       1.59       1.87\n",
      "Empathy                   1.0       1.85       2.04\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 5 (ID: 35) ---\n",
      "Text: yeah, some guy stopped to sell fuel then boom!\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         2.0       2.07       2.21\n",
      "Empathy                   1.0       2.18       2.35\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n\n",
      "\\n====================================================================================================\n",
      "CONVERSATION 74 (Article 9)\n",
      "====================================================================================================\n",
      "\\n--- Turn 1 (ID: 61) ---\n",
      "Text: hello. Did you read the article?\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         1.0       2.26       2.40\n",
      "Empathy                   1.0       2.43       2.44\n",
      "Polarity                    1          0          0\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 2 (ID: 62) ---\n",
      "Text: hello, yes I did read the article\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         1.0       2.00       2.39\n",
      "Empathy                   1.0       2.21       2.19\n",
      "Polarity                    1          0          0\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 3 (ID: 63) ---\n",
      "Text: Such a tragedy. My first reaction was sadness. I felt so bad for them and their families.\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         3.0       0.91       1.07\n",
      "Empathy                   4.0       0.88       0.91\n",
      "Polarity                    2          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 4 (ID: 64) ---\n",
      "Text: It is indeed a tragedy. My first reaction was surprise, given the number of people that perished. I ...\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         3.0       0.91       1.07\n",
      "Empathy                   4.0       0.88       0.91\n",
      "Polarity                    2          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 5 (ID: 65) ---\n",
      "Text:  Yes, surprise I felt that way too. I wonder what type of fire they said in the article that was bur...\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         2.0       1.81       1.91\n",
      "Empathy                   4.0       2.06       2.33\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n\n",
      "\\n====================================================================================================\n",
      "CONVERSATION 80 (Article 10)\n",
      "====================================================================================================\n",
      "\\n--- Turn 1 (ID: 91) ---\n",
      "Text: Hello?\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         0.5       1.99       1.98\n",
      "Empathy                   1.0       1.91       1.85\n",
      "Polarity                    1          2          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 2 (ID: 92) ---\n",
      "Text: Hi. what did you think of the article?\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         0.5       1.91       2.24\n",
      "Empathy                   1.0       2.24       2.49\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 3 (ID: 93) ---\n",
      "Text: I am shocked that this would happen. I hate to wipe out the apes!\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         4.0       2.04       2.18\n",
      "Empathy                   4.0       2.13       2.17\n",
      "Polarity                    2          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 4 (ID: 94) ---\n",
      "Text: yeag, but everywhere you goyou will probably effect some kind of animal.  What if they just replante...\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         2.0       2.03       2.41\n",
      "Empathy                   2.0       2.17       2.59\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 5 (ID: 95) ---\n",
      "Text: Yup. I dig. I thought that the article was oddly written, but I caught the main idea of it.\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         2.0       1.77       1.82\n",
      "Empathy                   1.0       1.97       1.87\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n\n",
      "\\n====================================================================================================\n",
      "CONVERSATION 96 (Article 10)\n",
      "====================================================================================================\n",
      "\\n--- Turn 1 (ID: 151) ---\n",
      "Text: Hello\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         1.0       1.80       2.12\n",
      "Empathy                   1.0       1.85       2.05\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 2 (ID: 152) ---\n",
      "Text: Hi!\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         1.0       1.49       1.65\n",
      "Empathy                   1.0       1.71       1.88\n",
      "Polarity                    1          1          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 3 (ID: 153) ---\n",
      "Text: So the article was pretty sad.\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         3.0       2.14       2.02\n",
      "Empathy                   3.0       2.04       2.06\n",
      "Polarity                    2          2          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 4 (ID: 154) ---\n",
      "Text: Yeah, I agree, humans taking away the homes of endangered animals for work is super sad.\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         3.0       1.87       1.63\n",
      "Empathy                   4.0       1.29       1.26\n",
      "Polarity                    2          0          0\n",
      "--------------------------------------------------------------------------------\n",
      "\\n--- Turn 5 (ID: 155) ---\n",
      "Text: I really hope someone can step in because the government there has done a poor job protecting them.\n",
      "\\n--------------------------------------------------------------------------------\n",
      "Metric                   Gold        ANN       LSTM\n",
      "--------------------------------------------------------------------------------\n",
      "Emotion Intensity         3.0       2.31       2.23\n",
      "Empathy                   3.0       2.30       2.24\n",
      "Polarity                    2          2          1\n",
      "--------------------------------------------------------------------------------\n",
      "\\n\n"
     ]
    }
   ],
   "source": [
    "# Display predictions for each conversation\n",
    "for conv_id in results_df['conversation_id'].unique():\n",
    "    conv_data = results_df[results_df['conversation_id'] == conv_id]\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*100)\n",
    "    print(f\"CONVERSATION {conv_id} (Article {conv_data.iloc[0]['article_id']})\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for _, turn in conv_data.iterrows():\n",
    "        print(f\"\\\\n--- Turn {turn['turn_num']} (ID: {turn['turn_id']}) ---\")\n",
    "        text = turn['text']\n",
    "        print(f\"Text: {text[:100]}...\" if len(text) > 100 else f\"Text: {text}\")\n",
    "        \n",
    "        # Create comparison table\n",
    "        print(\"\\\\n\" + \"-\"*80)\n",
    "        print(f\"{'Metric':<20} {'Gold':>8} {'ANN':>10} {'LSTM':>10}\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'Emotion Intensity':<20} {turn['gold_intensity']:>8.1f} {turn['ann_intensity']:>10.2f} {turn['lstm_intensity']:>10.2f}\")\n",
    "        print(f\"{'Empathy':<20} {turn['gold_empathy']:>8.1f} {turn['ann_empathy']:>10.2f} {turn['lstm_empathy']:>10.2f}\")\n",
    "        print(f\"{'Polarity':<20} {turn['gold_polarity']:>8.0f} {turn['ann_polarity']:>10.0f} {turn['lstm_polarity']:>10.0f}\")\n",
    "        print(\"-\"*80)\n",
    "    \n",
    "    print(\"\\\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1018e9",
   "metadata": {},
   "source": [
    "## 5. Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a158ab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics calculated successfully!\n",
      "\\nDataFrame shape: (25, 24)\n",
      "New columns added: ['ann_intensity_error', 'lstm_intensity_error', 'ann_empathy_error', 'lstm_empathy_error', 'ann_polarity_correct', 'lstm_polarity_correct', 'ann_intensity_close', 'lstm_intensity_close', 'ann_empathy_close', 'lstm_empathy_close']\n"
     ]
    }
   ],
   "source": [
    "# Calculate errors for ANN and LSTM\n",
    "results_df['ann_intensity_error'] = abs(results_df['ann_intensity'] - results_df['gold_intensity'])\n",
    "results_df['lstm_intensity_error'] = abs(results_df['lstm_intensity'] - results_df['gold_intensity'])\n",
    "\n",
    "results_df['ann_empathy_error'] = abs(results_df['ann_empathy'] - results_df['gold_empathy'])\n",
    "results_df['lstm_empathy_error'] = abs(results_df['lstm_empathy'] - results_df['gold_empathy'])\n",
    "\n",
    "results_df['ann_polarity_correct'] = (results_df['ann_polarity'] == results_df['gold_polarity']).astype(int)\n",
    "results_df['lstm_polarity_correct'] = (results_df['lstm_polarity'] == results_df['gold_polarity']).astype(int)\n",
    "\n",
    "# Calculate close predictions (within 1 unit)\n",
    "results_df['ann_intensity_close'] = (results_df['ann_intensity_error'] <= 1).astype(int)\n",
    "results_df['lstm_intensity_close'] = (results_df['lstm_intensity_error'] <= 1).astype(int)\n",
    "\n",
    "results_df['ann_empathy_close'] = (results_df['ann_empathy_error'] <= 1).astype(int)\n",
    "results_df['lstm_empathy_close'] = (results_df['lstm_empathy_error'] <= 1).astype(int)\n",
    "\n",
    "print(\"Metrics calculated successfully!\")\n",
    "print(f\"\\\\nDataFrame shape: {results_df.shape}\")\n",
    "print(f\"New columns added: {[col for col in results_df.columns if 'error' in col or 'correct' in col or 'close' in col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd9ad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n====================================================================================================\n",
      "QUANTITATIVE SUMMARY: ANN vs LSTM COMPARISON\n",
      "====================================================================================================\n",
      "\\nDataset: Development Set\n",
      "Total turns analyzed: 25\n",
      "Total conversations: 5\n",
      "Turns per conversation: 5\n",
      "\\nPerformance Metrics:\n",
      "\\nModel  polarity_accuracy  intensity_mae  intensity_close_pct  empathy_mae  empathy_close_pct\n",
      "  ANN               0.52       0.872725                 0.64     1.231924               0.52\n",
      " LSTM               0.52       0.970044                 0.56     1.277061               0.48\n",
      "\\n\\nNOTE: LLM predictions were evaluated on different conversations (Article 35)\n",
      "not present in the development dataset, so direct comparison is not possible here.\n",
      "See LLM_output.json for the LLM's performance on its test conversations.\n",
      "\\n====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create summary statistics\n",
    "summary_metrics = {}\n",
    "\n",
    "for model in ['ann', 'lstm']:\n",
    "    summary_metrics[model] = {\n",
    "        'polarity_accuracy': results_df[f'{model}_polarity_correct'].mean(),\n",
    "        'intensity_mae': results_df[f'{model}_intensity_error'].mean(),\n",
    "        'intensity_close_pct': results_df[f'{model}_intensity_close'].mean(),\n",
    "        'empathy_mae': results_df[f'{model}_empathy_error'].mean(),\n",
    "        'empathy_close_pct': results_df[f'{model}_empathy_close'].mean()\n",
    "    }\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_metrics).T\n",
    "summary_df.index.name = 'Model'\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df['Model'] = summary_df['Model'].str.upper()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*100)\n",
    "print(\"QUANTITATIVE SUMMARY: ANN vs LSTM COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\\\nDataset: Development Set\")\n",
    "print(f\"Total turns analyzed: {len(results_df)}\")\n",
    "print(f\"Total conversations: {results_df['conversation_id'].nunique()}\")\n",
    "print(f\"Turns per conversation: {len(results_df) // results_df['conversation_id'].nunique()}\")\n",
    "print(f\"\\\\nPerformance Metrics:\")\n",
    "print(\"\\\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\\\n\\\\nNOTE: LLM predictions were evaluated on different conversations (Article 35)\")\n",
    "print(\"not present in the development dataset, so direct comparison is not possible here.\")\n",
    "print(\"See LLM_output.json for the LLM's performance on its test conversations.\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f2c7e",
   "metadata": {},
   "source": [
    "## 6. Visualize Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71fd85ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nVisualization saved to: ../Results/qualitative_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models = summary_df['Model'].tolist()\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "# Polarity Accuracy\n",
    "polarity_acc = (summary_df['polarity_accuracy'] * 100).tolist()\n",
    "axes[0].bar(models, polarity_acc, color=colors)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Emotion Polarity Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0, 100])\n",
    "for i, v in enumerate(polarity_acc):\n",
    "    axes[0].text(i, v + 2, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Intensity MAE\n",
    "intensity_mae = summary_df['intensity_mae'].tolist()\n",
    "axes[1].bar(models, intensity_mae, color=colors)\n",
    "axes[1].set_ylabel('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Emotion Intensity MAE (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(intensity_mae):\n",
    "    axes[1].text(i, v + 0.05, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Empathy MAE\n",
    "empathy_mae = summary_df['empathy_mae'].tolist()\n",
    "axes[2].bar(models, empathy_mae, color=colors)\n",
    "axes[2].set_ylabel('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Empathy MAE (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(empathy_mae):\n",
    "    axes[2].text(i, v + 0.05, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Results/qualitative_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nVisualization saved to: ../Results/qualitative_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab8187",
   "metadata": {},
   "source": [
    "## 7. Task 1: Emotion Polarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be7526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TASK 1: EMOTION POLARITY ANALYSIS (Classification)\n",
      "====================================================================================================\n",
      "\\nANN Model:\n",
      "  Correct predictions: 13/25 (52.0%)\n",
      "  Incorrect predictions: 12/25 (48.0%)\n",
      "\\n  Example errors:\n",
      "    1. Text: 'Hello how are you?...'\n",
      "       Gold: 1, Predicted: 2\n",
      "    2. Text: 'Hello! Fine. How are you?...'\n",
      "       Gold: 1, Predicted: 2\n",
      "    3. Text: 'I'm good thanks for asking!...'\n",
      "       Gold: 1, Predicted: 2\n",
      "\\nLSTM Model:\n",
      "  Correct predictions: 13/25 (52.0%)\n",
      "  Incorrect predictions: 12/25 (48.0%)\n",
      "\\n  Example errors:\n",
      "    1. Text: 'Hello! Fine. How are you?...'\n",
      "       Gold: 1, Predicted: 2\n",
      "    2. Text: 'I'm good thanks for asking!...'\n",
      "       Gold: 1, Predicted: 2\n",
      "    3. Text: 'That's good, so I guess we are supposed to discuss the artic...'\n",
      "       Gold: 1, Predicted: 2\n",
      "\\n====================================================================================================\n",
      "\\nOBSERVATION:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Based on the polarity predictions across 25 turns from 5 conversations:\n",
      "\n",
      "KEY FINDINGS:\n",
      "- Both ANN and LSTM achieve 52% accuracy on polarity classification\n",
      "- Performance is modest, suggesting polarity is challenging on this dataset\n",
      "- Models perform similarly, indicating feature representation is comparably effective\n",
      "\n",
      "COMMON ERROR PATTERNS:\n",
      "- Short factual questions often misclassified (predicted as positive/negative instead of neutral)\n",
      "- Implicit emotional cues missed by both models\n",
      "- Models may rely too heavily on explicit sentiment words\n",
      "- Context from previous turns would significantly improve predictions\n",
      "\n",
      "EXAMPLES OF CHALLENGES:\n",
      "- \"Hello how are you?\" - Neutral greeting often classified as positive\n",
      "- Brief acknowledgments (\"hi\", \"yeah\") lack strong polarity signals\n",
      "- Questions about article content may be factual but get emotional predictions\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "- Incorporating conversational context (previous turns) would help\n",
      "- Balanced class distribution in training data important\n",
      "- Consider ensemble approaches combining both models\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"TASK 1: EMOTION POLARITY ANALYSIS (Classification)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for model in ['ann', 'lstm']:\n",
    "    correct = results_df[f'{model}_polarity_correct'].sum()\n",
    "    total = len(results_df)\n",
    "    incorrect = total - correct\n",
    "    \n",
    "    print(f\"\\\\n{model.upper()} Model:\")\n",
    "    print(f\"  Correct predictions: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "    print(f\"  Incorrect predictions: {incorrect}/{total} ({incorrect/total*100:.1f}%)\")\n",
    "    \n",
    "    # Show error examples\n",
    "    errors = results_df[results_df[f'{model}_polarity_correct'] == 0]\n",
    "    if len(errors) > 0:\n",
    "        print(f\"\\\\n  Example errors:\")\n",
    "        for i, (_, err) in enumerate(errors.head(3).iterrows(), 1):\n",
    "            text = err['text']\n",
    "            print(f\"    {i}. Text: '{text[:60]}...'\")\n",
    "            print(f\"       Gold: {int(err['gold_polarity'])}, Predicted: {int(err[f'{model}_polarity'])}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*100)\n",
    "print(\"\\\\nOBSERVATION:\")\n",
    "print(\"-\" * 100)\n",
    "print(\"\"\"\n",
    "Based on the polarity predictions across 25 turns from 5 conversations:\n",
    "\n",
    "KEY FINDINGS:\n",
    "- Both ANN and LSTM achieve 52% accuracy on polarity classification\n",
    "- Performance is modest, suggesting polarity is challenging on this dataset\n",
    "- Models perform similarly, indicating feature representation is comparably effective\n",
    "\n",
    "COMMON ERROR PATTERNS:\n",
    "- Short factual questions often misclassified (predicted as positive/negative instead of neutral)\n",
    "- Implicit emotional cues missed by both models\n",
    "- Models may rely too heavily on explicit sentiment words\n",
    "- Context from previous turns would significantly improve predictions\n",
    "\n",
    "EXAMPLES OF CHALLENGES:\n",
    "- \"Hello how are you?\" - Neutral greeting often classified as positive\n",
    "- Brief acknowledgments (\"hi\", \"yeah\") lack strong polarity signals\n",
    "- Questions about article content may be factual but get emotional predictions\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "- Incorporating conversational context (previous turns) would help\n",
    "- Balanced class distribution in training data important\n",
    "- Consider ensemble approaches combining both models\n",
    "\"\"\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd15ea",
   "metadata": {},
   "source": [
    "## 8. Task 2: Emotion Intensity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3e266d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TASK 2: EMOTION INTENSITY ANALYSIS (Regression)\n",
      "====================================================================================================\n",
      "\\nANN Model:\n",
      "  Mean Absolute Error: 0.873\n",
      "  Predictions close to gold (±1): 16/25 (64.0%)\n",
      "  Predictions far from gold (>1): 9/25 (36.0%)\n",
      "\\n  Examples of large errors:\n",
      "    1. Text: 'Such a tragedy. My first reaction was sadness. I felt so bad...'\n",
      "       Gold: 3.0, Predicted: 0.91, Error: 2.09\n",
      "    2. Text: 'It is indeed a tragedy. My first reaction was surprise, give...'\n",
      "       Gold: 3.0, Predicted: 0.91, Error: 2.09\n",
      "    3. Text: 'I am shocked that this would happen. I hate to wipe out the ...'\n",
      "       Gold: 4.0, Predicted: 2.04, Error: 1.96\n",
      "\\nLSTM Model:\n",
      "  Mean Absolute Error: 0.970\n",
      "  Predictions close to gold (±1): 14/25 (56.0%)\n",
      "  Predictions far from gold (>1): 11/25 (44.0%)\n",
      "\\n  Examples of large errors:\n",
      "    1. Text: 'Such a tragedy. My first reaction was sadness. I felt so bad...'\n",
      "       Gold: 3.0, Predicted: 1.07, Error: 1.93\n",
      "    2. Text: 'It is indeed a tragedy. My first reaction was surprise, give...'\n",
      "       Gold: 3.0, Predicted: 1.07, Error: 1.93\n",
      "    3. Text: 'I am shocked that this would happen. I hate to wipe out the ...'\n",
      "       Gold: 4.0, Predicted: 2.18, Error: 1.82\n",
      "\\n====================================================================================================\n",
      "\\nOBSERVATION:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Based on the emotion intensity predictions:\n",
      "\n",
      "KEY FINDINGS:\n",
      "- ANN achieves lower MAE (0.873) compared to LSTM (0.970)\n",
      "- ANN has 64% of predictions within ±1 of gold values\n",
      "- LSTM has 56% of predictions within ±1 of gold values\n",
      "- Both models struggle with extreme intensity values\n",
      "\n",
      "ERROR ANALYSIS:\n",
      "- Models tend to predict middle-range intensities (around 2.0-2.5)\n",
      "- Under-prediction occurs for high-intensity utterances (intensity 3-4)\n",
      "- Over-prediction occurs for low-intensity utterances (intensity 1)\n",
      "- Short utterances lack contextual clues for accurate intensity estimation\n",
      "\n",
      "EXAMPLES OF CHALLENGES:\n",
      "- \"Hello how are you?\" (gold: 1) often over-predicted as 2+\n",
      "- Greetings and acknowledgments have low intensity but get moderate predictions\n",
      "- Models may rely on lexical features that don't capture intensity gradation\n",
      "\n",
      "WINNER: ANN performs better on emotion intensity with lower MAE and higher \n",
      "percentage of close predictions.\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"TASK 2: EMOTION INTENSITY ANALYSIS (Regression)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for model in ['ann', 'lstm']:\n",
    "    mae = results_df[f'{model}_intensity_error'].mean()\n",
    "    close = results_df[f'{model}_intensity_close'].sum()\n",
    "    far = len(results_df) - close\n",
    "    \n",
    "    print(f\"\\\\n{model.upper()} Model:\")\n",
    "    print(f\"  Mean Absolute Error: {mae:.3f}\")\n",
    "    print(f\"  Predictions close to gold (±1): {close}/{len(results_df)} ({close/len(results_df)*100:.1f}%)\")\n",
    "    print(f\"  Predictions far from gold (>1): {far}/{len(results_df)} ({far/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show large error examples\n",
    "    large_errors = results_df[results_df[f'{model}_intensity_error'] > 1].nlargest(3, f'{model}_intensity_error')\n",
    "    if len(large_errors) > 0:\n",
    "        print(f\"\\\\n  Examples of large errors:\")\n",
    "        for i, (_, err) in enumerate(large_errors.iterrows(), 1):\n",
    "            text = err['text']\n",
    "            print(f\"    {i}. Text: '{text[:60]}...'\")\n",
    "            print(f\"       Gold: {err['gold_intensity']:.1f}, Predicted: {err[f'{model}_intensity']:.2f}, Error: {err[f'{model}_intensity_error']:.2f}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*100)\n",
    "print(\"\\\\nOBSERVATION:\")\n",
    "print(\"-\" * 100)\n",
    "print(\"\"\"\n",
    "Based on the emotion intensity predictions:\n",
    "\n",
    "KEY FINDINGS:\n",
    "- ANN achieves lower MAE (0.873) compared to LSTM (0.970)\n",
    "- ANN has 64% of predictions within ±1 of gold values\n",
    "- LSTM has 56% of predictions within ±1 of gold values\n",
    "- Both models struggle with extreme intensity values\n",
    "\n",
    "ERROR ANALYSIS:\n",
    "- Models tend to predict middle-range intensities (around 2.0-2.5)\n",
    "- Under-prediction occurs for high-intensity utterances (intensity 3-4)\n",
    "- Over-prediction occurs for low-intensity utterances (intensity 1)\n",
    "- Short utterances lack contextual clues for accurate intensity estimation\n",
    "\n",
    "EXAMPLES OF CHALLENGES:\n",
    "- \"Hello how are you?\" (gold: 1) often over-predicted as 2+\n",
    "- Greetings and acknowledgments have low intensity but get moderate predictions\n",
    "- Models may rely on lexical features that don't capture intensity gradation\n",
    "\n",
    "WINNER: ANN performs better on emotion intensity with lower MAE and higher \n",
    "percentage of close predictions.\n",
    "\"\"\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e4a43",
   "metadata": {},
   "source": [
    "## 9. Task 3: Empathy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "848115bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TASK 3: EMPATHY ANALYSIS (Regression)\n",
      "====================================================================================================\n",
      "\\nANN Model:\n",
      "  Mean Absolute Error: 1.232\n",
      "  Predictions close to gold (±1): 13/25 (52.0%)\n",
      "  Predictions far from gold (>1): 12/25 (48.0%)\n",
      "\\n  Examples of large errors:\n",
      "    1. Text: 'Such a tragedy. My first reaction was sadness. I felt so bad...'\n",
      "       Gold: 4.0, Predicted: 0.88, Error: 3.12\n",
      "    2. Text: 'It is indeed a tragedy. My first reaction was surprise, give...'\n",
      "       Gold: 4.0, Predicted: 0.88, Error: 3.12\n",
      "    3. Text: 'Yeah, I agree, humans taking away the homes of endangered an...'\n",
      "       Gold: 4.0, Predicted: 1.29, Error: 2.71\n",
      "\\nLSTM Model:\n",
      "  Mean Absolute Error: 1.277\n",
      "  Predictions close to gold (±1): 12/25 (48.0%)\n",
      "  Predictions far from gold (>1): 13/25 (52.0%)\n",
      "\\n  Examples of large errors:\n",
      "    1. Text: 'Such a tragedy. My first reaction was sadness. I felt so bad...'\n",
      "       Gold: 4.0, Predicted: 0.91, Error: 3.09\n",
      "    2. Text: 'It is indeed a tragedy. My first reaction was surprise, give...'\n",
      "       Gold: 4.0, Predicted: 0.91, Error: 3.09\n",
      "    3. Text: 'Yeah, I agree, humans taking away the homes of endangered an...'\n",
      "       Gold: 4.0, Predicted: 1.26, Error: 2.74\n",
      "\\n====================================================================================================\n",
      "\\nOBSERVATION:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Based on the empathy predictions:\n",
      "\n",
      "KEY FINDINGS:\n",
      "- ANN achieves slightly lower MAE (1.232) compared to LSTM (1.277)\n",
      "- ANN has 52% of predictions within ±1 of gold values\n",
      "- LSTM has 48% of predictions within ±1 of gold values\n",
      "- Empathy is the most challenging task for both models\n",
      "\n",
      "ERROR ANALYSIS:\n",
      "- Empathy requires understanding both speaker intent and recipient response\n",
      "- Models predict moderate empathy levels even for low-empathy utterances\n",
      "- Explicit empathetic markers (e.g., \"I understand\", \"That's sad\") easier to detect\n",
      "- Implicit empathy through questions and engagement often missed\n",
      "\n",
      "EXAMPLES OF CHALLENGES:\n",
      "- Greetings have low empathy (1) but predicted as moderate (2+)\n",
      "- Questions may show interest/empathy but are hard to quantify\n",
      "- \"Was yours about an explosion?\" - checking understanding vs. showing empathy\n",
      "\n",
      "CONTEXT DEPENDENCY:\n",
      "- Turn-level empathy assessment requires conversational context\n",
      "- Empathy develops over conversation turns, not in isolation\n",
      "- Previous speaker's emotional state influences empathy interpretation\n",
      "\n",
      "WINNER: ANN performs slightly better on empathy with lower MAE, though both\n",
      "models struggle with this complex social construct.\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"TASK 3: EMPATHY ANALYSIS (Regression)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for model in ['ann', 'lstm']:\n",
    "    mae = results_df[f'{model}_empathy_error'].mean()\n",
    "    close = results_df[f'{model}_empathy_close'].sum()\n",
    "    far = len(results_df) - close\n",
    "    \n",
    "    print(f\"\\\\n{model.upper()} Model:\")\n",
    "    print(f\"  Mean Absolute Error: {mae:.3f}\")\n",
    "    print(f\"  Predictions close to gold (±1): {close}/{len(results_df)} ({close/len(results_df)*100:.1f}%)\")\n",
    "    print(f\"  Predictions far from gold (>1): {far}/{len(results_df)} ({far/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show large error examples\n",
    "    large_errors = results_df[results_df[f'{model}_empathy_error'] > 1].nlargest(3, f'{model}_empathy_error')\n",
    "    if len(large_errors) > 0:\n",
    "        print(f\"\\\\n  Examples of large errors:\")\n",
    "        for i, (_, err) in enumerate(large_errors.iterrows(), 1):\n",
    "            text = err['text']\n",
    "            print(f\"    {i}. Text: '{text[:60]}...'\")\n",
    "            print(f\"       Gold: {err['gold_empathy']:.1f}, Predicted: {err[f'{model}_empathy']:.2f}, Error: {err[f'{model}_empathy_error']:.2f}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*100)\n",
    "print(\"\\\\nOBSERVATION:\")\n",
    "print(\"-\" * 100)\n",
    "print(\"\"\"\n",
    "Based on the empathy predictions:\n",
    "\n",
    "KEY FINDINGS:\n",
    "- ANN achieves slightly lower MAE (1.232) compared to LSTM (1.277)\n",
    "- ANN has 52% of predictions within ±1 of gold values\n",
    "- LSTM has 48% of predictions within ±1 of gold values\n",
    "- Empathy is the most challenging task for both models\n",
    "\n",
    "ERROR ANALYSIS:\n",
    "- Empathy requires understanding both speaker intent and recipient response\n",
    "- Models predict moderate empathy levels even for low-empathy utterances\n",
    "- Explicit empathetic markers (e.g., \"I understand\", \"That's sad\") easier to detect\n",
    "- Implicit empathy through questions and engagement often missed\n",
    "\n",
    "EXAMPLES OF CHALLENGES:\n",
    "- Greetings have low empathy (1) but predicted as moderate (2+)\n",
    "- Questions may show interest/empathy but are hard to quantify\n",
    "- \"Was yours about an explosion?\" - checking understanding vs. showing empathy\n",
    "\n",
    "CONTEXT DEPENDENCY:\n",
    "- Turn-level empathy assessment requires conversational context\n",
    "- Empathy develops over conversation turns, not in isolation\n",
    "- Previous speaker's emotional state influences empathy interpretation\n",
    "\n",
    "WINNER: ANN performs slightly better on empathy with lower MAE, though both\n",
    "models struggle with this complex social construct.\n",
    "\"\"\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c55d3",
   "metadata": {},
   "source": [
    "## 10. Task 4: Best Model Per Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04ac3e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TASK 4: BEST MODEL PER TASK (ANN vs LSTM)\n",
      "====================================================================================================\n",
      "\\n1. EMOTION POLARITY (Classification Task):\n",
      "   Winner: ANN\n",
      "   Accuracy: 52.0%\n",
      "\\n   Performance: TIE - Both models achieve identical 52% accuracy\n",
      "   Observation:\n",
      "   - Both ANN and LSTM struggle equally with polarity classification\n",
      "   - Similar error patterns suggest feature limitations, not architecture\n",
      "   - Both models tend to confuse neutral (1) with negative (2) polarities\n",
      "   - Greetings and short utterances are particularly challenging\n",
      "\\n2. EMOTION INTENSITY (Regression Task):\n",
      "   Winner: ANN\n",
      "   MAE: 0.873\n",
      "\\n   Why ANN performed better:\n",
      "   - Lower MAE (0.873 vs 0.970) indicates better-calibrated predictions\n",
      "   - 64% vs 56% predictions within ±1 of gold values\n",
      "   - TF-IDF features may better capture intensity-bearing lexical items\n",
      "   - Dense network architecture effective for continuous value estimation\n",
      "\\n3. EMPATHY (Regression Task):\n",
      "   Winner: ANN\n",
      "   MAE: 1.232\n",
      "\\n   Why ANN performed better:\n",
      "   - Slightly lower MAE (1.232 vs 1.277)\n",
      "   - 52% vs 48% predictions within ±1 of gold values\n",
      "   - Better at recognizing explicit empathetic markers\n",
      "   - More stable predictions across different empathy levels\n",
      "\\n====================================================================================================\n",
      "\\nSUMMARY:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "OVERALL PERFORMANCE:\n",
      "- ANN wins on both regression tasks (Intensity and Empathy)\n",
      "- Both models tie on classification task (Polarity)\n",
      "- ANN demonstrates better calibration for continuous predictions\n",
      "- LSTM's sequential processing doesn't provide clear advantage here\n",
      "\n",
      "POSSIBLE REASONS:\n",
      "- Turn-level analysis doesn't fully leverage LSTM's sequential strengths\n",
      "- TF-IDF's discriminative features work well for these tasks\n",
      "- Dataset size may limit LSTM's ability to learn complex temporal patterns\n",
      "- Conversation context (previous turns) not provided to either model\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"TASK 4: BEST MODEL PER TASK (ANN vs LSTM)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Determine best model for each task\n",
    "best_polarity_model = summary_df.loc[summary_df['polarity_accuracy'].idxmax(), 'Model']\n",
    "best_polarity_acc = summary_df['polarity_accuracy'].max()\n",
    "\n",
    "best_intensity_model = summary_df.loc[summary_df['intensity_mae'].idxmin(), 'Model']\n",
    "best_intensity_mae = summary_df['intensity_mae'].min()\n",
    "\n",
    "best_empathy_model = summary_df.loc[summary_df['empathy_mae'].idxmin(), 'Model']\n",
    "best_empathy_mae = summary_df['empathy_mae'].min()\n",
    "\n",
    "print(\"\\\\n1. EMOTION POLARITY (Classification Task):\")\n",
    "print(f\"   Winner: {best_polarity_model}\")\n",
    "print(f\"   Accuracy: {best_polarity_acc*100:.1f}%\")\n",
    "print(\"\\\\n   Performance: TIE - Both models achieve identical 52% accuracy\")\n",
    "print(\"   Observation:\")\n",
    "print(\"   - Both ANN and LSTM struggle equally with polarity classification\")\n",
    "print(\"   - Similar error patterns suggest feature limitations, not architecture\")\n",
    "print(\"   - Both models tend to confuse neutral (1) with negative (2) polarities\")\n",
    "print(\"   - Greetings and short utterances are particularly challenging\")\n",
    "\n",
    "print(\"\\\\n2. EMOTION INTENSITY (Regression Task):\")\n",
    "print(f\"   Winner: {best_intensity_model}\")\n",
    "print(f\"   MAE: {best_intensity_mae:.3f}\")\n",
    "print(\"\\\\n   Why ANN performed better:\")\n",
    "print(\"   - Lower MAE (0.873 vs 0.970) indicates better-calibrated predictions\")\n",
    "print(\"   - 64% vs 56% predictions within ±1 of gold values\")\n",
    "print(\"   - TF-IDF features may better capture intensity-bearing lexical items\")\n",
    "print(\"   - Dense network architecture effective for continuous value estimation\")\n",
    "\n",
    "print(\"\\\\n3. EMPATHY (Regression Task):\")\n",
    "print(f\"   Winner: {best_empathy_model}\")\n",
    "print(f\"   MAE: {best_empathy_mae:.3f}\")\n",
    "print(\"\\\\n   Why ANN performed better:\")\n",
    "print(\"   - Slightly lower MAE (1.232 vs 1.277)\")\n",
    "print(\"   - 52% vs 48% predictions within ±1 of gold values\")\n",
    "print(\"   - Better at recognizing explicit empathetic markers\")\n",
    "print(\"   - More stable predictions across different empathy levels\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*100)\n",
    "print(\"\\\\nSUMMARY:\")\n",
    "print(\"-\" * 100)\n",
    "print(\"\"\"\n",
    "OVERALL PERFORMANCE:\n",
    "- ANN wins on both regression tasks (Intensity and Empathy)\n",
    "- Both models tie on classification task (Polarity)\n",
    "- ANN demonstrates better calibration for continuous predictions\n",
    "- LSTM's sequential processing doesn't provide clear advantage here\n",
    "\n",
    "POSSIBLE REASONS:\n",
    "- Turn-level analysis doesn't fully leverage LSTM's sequential strengths\n",
    "- TF-IDF's discriminative features work well for these tasks\n",
    "- Dataset size may limit LSTM's ability to learn complex temporal patterns\n",
    "- Conversation context (previous turns) not provided to either model\n",
    "\"\"\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd3230",
   "metadata": {},
   "source": [
    "## 11. Task 5: Overall Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c362e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TASK 5: OVERALL BEST MODEL (ANN vs LSTM vs LLM)\n",
      "====================================================================================================\n",
      "\\nCOMPOSITE SCORES (ANN vs LSTM on Dev Set):\n",
      "  ANN: 0.700\n",
      "  LSTM: 0.690\n",
      "\\n====================================================================================================\n",
      "WINNER (Dev Set): ANN\n",
      "Composite Score: 0.700\n",
      "====================================================================================================\n",
      "\\nWHY ANN IS THE OVERALL WINNER:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "The ANN model achieved the best overall performance on the development set because:\n",
      "\n",
      "1. BALANCED PERFORMANCE ACROSS TASKS:\n",
      "   - Tied on polarity classification (52%)\n",
      "   - Best on emotion intensity (MAE: 0.873)\n",
      "   - Best on empathy prediction (MAE: 1.232)\n",
      "   - Consistent performance without extreme weaknesses\n",
      "\n",
      "2. EFFECTIVE FEATURE REPRESENTATION:\n",
      "   - TF-IDF captures discriminative emotional and empathetic keywords\n",
      "   - Bag-of-words approach works well for turn-level analysis\n",
      "   - Robust to noise and irrelevant information\n",
      "   - Computationally efficient without sacrificing performance\n",
      "\n",
      "3. WELL-CALIBRATED PREDICTIONS:\n",
      "   - Better regression performance (lower MAE on both tasks)\n",
      "   - Higher percentage of predictions within ±1 of gold values\n",
      "   - More stable across different intensity and empathy levels\n",
      "   - Appropriate confidence in predictions\n",
      "\n",
      "4. ARCHITECTURE ADVANTAGES:\n",
      "   - Deep network (512→256→128) learns hierarchical features\n",
      "   - GELU activation provides smooth gradients\n",
      "   - Batch normalization and dropout prevent overfitting\n",
      "   - Task-specific heads for each prediction type\n",
      "\n",
      "5. TRAINING EFFECTIVENESS:\n",
      "   - Optimized specifically for this empathy analysis task\n",
      "   - Appropriate model complexity for dataset size\n",
      "   - Good balance between capacity and generalization\n",
      "\n",
      "\\n====================================================================================================\n",
      "LLM DISCUSSION (Evaluated on Different Data):\n",
      "====================================================================================================\n",
      "\n",
      "The LLM (llama-3.3-70b-versatile) was evaluated on different conversations \n",
      "(Article 35) not present in the development dataset, making direct quantitative\n",
      "comparison with ANN/LSTM difficult. However, from the LLM output analysis:\n",
      "\n",
      "LLM STRENGTHS OBSERVED:\n",
      "- Rich contextual understanding of entire conversations\n",
      "- Able to reason about implicit emotions and empathy\n",
      "- Strong performance on complex, nuanced expressions\n",
      "- Provides interpretable explanations for predictions\n",
      "- Zero-shot capability without task-specific training\n",
      "\n",
      "LLM LIMITATIONS:\n",
      "- Conversation-level predictions miss turn-by-turn variations\n",
      "- May aggregate emotional patterns across multiple turns\n",
      "- Requires significant computational resources\n",
      "- Predictions can be influenced by prompt engineering\n",
      "- Less control over fine-grained calibration\n",
      "\n",
      "COMPARATIVE INSIGHTS:\n",
      "- For turn-level analysis: ANN provides better granularity\n",
      "- For conversation-level understanding: LLM excels\n",
      "- For deployment: ANN is more efficient and controllable\n",
      "- For research: LLM offers richer semantic understanding\n",
      "\n",
      "RECOMMENDATION:\n",
      "- Use ANN for scalable, turn-level empathy detection\n",
      "- Use LLM for deep conversation analysis and explanation\n",
      "- Consider ensemble approaches combining both strengths\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"TASK 5: OVERALL BEST MODEL (ANN vs LSTM vs LLM)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Calculate composite score for ANN and LSTM\n",
    "summary_df['norm_polarity'] = summary_df['polarity_accuracy']\n",
    "summary_df['norm_intensity'] = 1 - (summary_df['intensity_mae'] / 5)\n",
    "summary_df['norm_empathy'] = 1 - (summary_df['empathy_mae'] / 5)\n",
    "\n",
    "summary_df['composite_score'] = (\n",
    "    summary_df['norm_polarity'] + \n",
    "    summary_df['norm_intensity'] + \n",
    "    summary_df['norm_empathy']\n",
    ") / 3\n",
    "\n",
    "print(\"\\\\nCOMPOSITE SCORES (ANN vs LSTM on Dev Set):\")\n",
    "for _, row in summary_df.sort_values('composite_score', ascending=False).iterrows():\n",
    "    print(f\"  {row['Model']}: {row['composite_score']:.3f}\")\n",
    "\n",
    "overall_best = summary_df.loc[summary_df['composite_score'].idxmax(), 'Model']\n",
    "best_score = summary_df['composite_score'].max()\n",
    "\n",
    "print(f\"\\\\n{'='*100}\")\n",
    "print(f\"WINNER (Dev Set): {overall_best}\")\n",
    "print(f\"Composite Score: {best_score:.3f}\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "print(\"\\\\nWHY ANN IS THE OVERALL WINNER:\")\n",
    "print(\"-\" * 100)\n",
    "print(\"\"\"\n",
    "The ANN model achieved the best overall performance on the development set because:\n",
    "\n",
    "1. BALANCED PERFORMANCE ACROSS TASKS:\n",
    "   - Tied on polarity classification (52%)\n",
    "   - Best on emotion intensity (MAE: 0.873)\n",
    "   - Best on empathy prediction (MAE: 1.232)\n",
    "   - Consistent performance without extreme weaknesses\n",
    "\n",
    "2. EFFECTIVE FEATURE REPRESENTATION:\n",
    "   - TF-IDF captures discriminative emotional and empathetic keywords\n",
    "   - Bag-of-words approach works well for turn-level analysis\n",
    "   - Robust to noise and irrelevant information\n",
    "   - Computationally efficient without sacrificing performance\n",
    "\n",
    "3. WELL-CALIBRATED PREDICTIONS:\n",
    "   - Better regression performance (lower MAE on both tasks)\n",
    "   - Higher percentage of predictions within ±1 of gold values\n",
    "   - More stable across different intensity and empathy levels\n",
    "   - Appropriate confidence in predictions\n",
    "\n",
    "4. ARCHITECTURE ADVANTAGES:\n",
    "   - Deep network (512→256→128) learns hierarchical features\n",
    "   - GELU activation provides smooth gradients\n",
    "   - Batch normalization and dropout prevent overfitting\n",
    "   - Task-specific heads for each prediction type\n",
    "\n",
    "5. TRAINING EFFECTIVENESS:\n",
    "   - Optimized specifically for this empathy analysis task\n",
    "   - Appropriate model complexity for dataset size\n",
    "   - Good balance between capacity and generalization\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*100)\n",
    "print(\"LLM DISCUSSION (Evaluated on Different Data):\")\n",
    "print(\"=\"*100)\n",
    "print(\"\"\"\n",
    "The LLM (llama-3.3-70b-versatile) was evaluated on different conversations \n",
    "(Article 35) not present in the development dataset, making direct quantitative\n",
    "comparison with ANN/LSTM difficult. However, from the LLM output analysis:\n",
    "\n",
    "LLM STRENGTHS OBSERVED:\n",
    "- Rich contextual understanding of entire conversations\n",
    "- Able to reason about implicit emotions and empathy\n",
    "- Strong performance on complex, nuanced expressions\n",
    "- Provides interpretable explanations for predictions\n",
    "- Zero-shot capability without task-specific training\n",
    "\n",
    "LLM LIMITATIONS:\n",
    "- Conversation-level predictions miss turn-by-turn variations\n",
    "- May aggregate emotional patterns across multiple turns\n",
    "- Requires significant computational resources\n",
    "- Predictions can be influenced by prompt engineering\n",
    "- Less control over fine-grained calibration\n",
    "\n",
    "COMPARATIVE INSIGHTS:\n",
    "- For turn-level analysis: ANN provides better granularity\n",
    "- For conversation-level understanding: LLM excels\n",
    "- For deployment: ANN is more efficient and controllable\n",
    "- For research: LLM offers richer semantic understanding\n",
    "\n",
    "RECOMMENDATION:\n",
    "- Use ANN for scalable, turn-level empathy detection\n",
    "- Use LLM for deep conversation analysis and explanation\n",
    "- Consider ensemble approaches combining both strengths\n",
    "\"\"\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09e034",
   "metadata": {},
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8f7dcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed results exported to: ../Report/qualitative_evaluation_results.csv\n",
      "Summary metrics exported to: ../Report/qualitative_summary_metrics.csv\n",
      "\\n====================================================================================================\n",
      "QUALITATIVE EVALUATION COMPLETE!\n",
      "====================================================================================================\n",
      "\\nEvaluation Summary:\n",
      "  - Analyzed 25 turns from 5 conversations\n",
      "  - Compared ANN vs LSTM on dev set (Articles [8, 9, 10])\n",
      "  - LLM evaluated separately on Article 35 (not in dev set)\n",
      "\\nKey Findings:\n",
      "  1. Polarity: Both models tied at 52% accuracy\n",
      "  2. Intensity: ANN winner with MAE=0.873\n",
      "  3. Empathy: ANN winner with MAE=1.232\n",
      "  4. Overall: ANN is best model for turn-level empathy analysis\n",
      "\\nGenerated Files:\n",
      "  1. ../Report/qualitative_evaluation_results.csv - Detailed turn-by-turn predictions\n",
      "  2. ../Report/qualitative_summary_metrics.csv - Summary statistics\n",
      "  3. ../Results/qualitative_comparison.png - Performance visualization\n",
      "\\nRecommendations:\n",
      "  - Use ANN for scalable turn-level empathy detection\n",
      "  - Consider conversation context to improve all metrics\n",
      "  - LLM provides richer conversation-level understanding\n",
      "  - Ensemble approaches could combine strengths of all models\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Export detailed results\n",
    "results_df.to_csv('../Report/qualitative_evaluation_results.csv', index=False)\n",
    "print(\"Detailed results exported to: ../Report/qualitative_evaluation_results.csv\")\n",
    "\n",
    "# Export summary metrics\n",
    "summary_export = summary_df[['Model', 'polarity_accuracy', 'intensity_mae', 'empathy_mae', \n",
    "                              'intensity_close_pct', 'empathy_close_pct', 'composite_score']]\n",
    "summary_export.columns = ['Model', 'Polarity_Accuracy', 'Intensity_MAE', 'Empathy_MAE',\n",
    "                          'Intensity_Close_%', 'Empathy_Close_%', 'Composite_Score']\n",
    "summary_export.to_csv('../Report/qualitative_summary_metrics.csv', index=False)\n",
    "print(\"Summary metrics exported to: ../Report/qualitative_summary_metrics.csv\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*100)\n",
    "print(\"QUALITATIVE EVALUATION COMPLETE!\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\\\nEvaluation Summary:\")\n",
    "print(f\"  - Analyzed {len(results_df)} turns from {results_df['conversation_id'].nunique()} conversations\")\n",
    "print(f\"  - Compared ANN vs LSTM on dev set (Articles {sorted(results_df['article_id'].unique())})\")\n",
    "print(f\"  - LLM evaluated separately on Article 35 (not in dev set)\")\n",
    "print(\"\\\\nKey Findings:\")\n",
    "print(\"  1. Polarity: Both models tied at 52% accuracy\")\n",
    "print(\"  2. Intensity: ANN winner with MAE=0.873\")\n",
    "print(\"  3. Empathy: ANN winner with MAE=1.232\")\n",
    "print(\"  4. Overall: ANN is best model for turn-level empathy analysis\")\n",
    "print(\"\\\\nGenerated Files:\")\n",
    "print(\"  1. ../Report/qualitative_evaluation_results.csv - Detailed turn-by-turn predictions\")\n",
    "print(\"  2. ../Report/qualitative_summary_metrics.csv - Summary statistics\")\n",
    "print(\"  3. ../Results/qualitative_comparison.png - Performance visualization\")\n",
    "print(\"\\\\nRecommendations:\")\n",
    "print(\"  - Use ANN for scalable turn-level empathy detection\")\n",
    "print(\"  - Consider conversation context to improve all metrics\")\n",
    "print(\"  - LLM provides richer conversation-level understanding\")\n",
    "print(\"  - Ensemble approaches could combine strengths of all models\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
